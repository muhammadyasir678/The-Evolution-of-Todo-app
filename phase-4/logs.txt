
==> Audit <==
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    COMMAND     ‚îÇ                            ARGS                             ‚îÇ PROFILE  ‚îÇ    USER     ‚îÇ VERSION ‚îÇ     START TIME      ‚îÇ      END TIME       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ start          ‚îÇ                                                             ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 05 Feb 26 01:25 PKT ‚îÇ                     ‚îÇ
‚îÇ config         ‚îÇ set driver virtualbox                                       ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 05 Feb 26 01:27 PKT ‚îÇ 05 Feb 26 01:27 PKT ‚îÇ
‚îÇ delete         ‚îÇ                                                             ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 05 Feb 26 01:28 PKT ‚îÇ 05 Feb 26 01:28 PKT ‚îÇ
‚îÇ start          ‚îÇ                                                             ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 05 Feb 26 01:28 PKT ‚îÇ                     ‚îÇ
‚îÇ start          ‚îÇ                                                             ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 05 Feb 26 01:36 PKT ‚îÇ                     ‚îÇ
‚îÇ delete         ‚îÇ                                                             ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 05 Feb 26 01:36 PKT ‚îÇ 05 Feb 26 01:36 PKT ‚îÇ
‚îÇ start          ‚îÇ --driver=docker                                             ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 05 Feb 26 03:27 PKT ‚îÇ                     ‚îÇ
‚îÇ start          ‚îÇ                                                             ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 05 Feb 26 03:29 PKT ‚îÇ                     ‚îÇ
‚îÇ start          ‚îÇ                                                             ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 02:14 PKT ‚îÇ                     ‚îÇ
‚îÇ start          ‚îÇ --memory=8192 --cpus=4                                      ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 02:30 PKT ‚îÇ                     ‚îÇ
‚îÇ start          ‚îÇ --memory=2851mb                                             ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 02:31 PKT ‚îÇ                     ‚îÇ
‚îÇ start          ‚îÇ --memory=2851 --cpus=4                                      ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 02:31 PKT ‚îÇ                     ‚îÇ
‚îÇ start          ‚îÇ                                                             ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 02:38 PKT ‚îÇ                     ‚îÇ
‚îÇ start          ‚îÇ                                                             ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 02:40 PKT ‚îÇ                     ‚îÇ
‚îÇ start          ‚îÇ                                                             ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 02:43 PKT ‚îÇ                     ‚îÇ
‚îÇ start          ‚îÇ --driver=docker                                             ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 02:43 PKT ‚îÇ                     ‚îÇ
‚îÇ update-check   ‚îÇ                                                             ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 13:37 PKT ‚îÇ 09 Feb 26 13:37 PKT ‚îÇ
‚îÇ config         ‚îÇ set driver docker                                           ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 13:44 PKT ‚îÇ 09 Feb 26 13:44 PKT ‚îÇ
‚îÇ start          ‚îÇ                                                             ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 13:45 PKT ‚îÇ                     ‚îÇ
‚îÇ config         ‚îÇ set driver docker                                           ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 13:47 PKT ‚îÇ 09 Feb 26 13:47 PKT ‚îÇ
‚îÇ config         ‚îÇ set driver docker                                           ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 13:51 PKT ‚îÇ 09 Feb 26 13:51 PKT ‚îÇ
‚îÇ config         ‚îÇ set driver docker                                           ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 14:09 PKT ‚îÇ 09 Feb 26 14:09 PKT ‚îÇ
‚îÇ start          ‚îÇ                                                             ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 14:15 PKT ‚îÇ                     ‚îÇ
‚îÇ config         ‚îÇ set driver docker                                           ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 14:16 PKT ‚îÇ 09 Feb 26 14:16 PKT ‚îÇ
‚îÇ start          ‚îÇ                                                             ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 14:17 PKT ‚îÇ                     ‚îÇ
‚îÇ start          ‚îÇ --driver=docker                                             ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 14:26 PKT ‚îÇ                     ‚îÇ
‚îÇ config         ‚îÇ set driver docker                                           ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 14:27 PKT ‚îÇ 09 Feb 26 14:27 PKT ‚îÇ
‚îÇ start          ‚îÇ                                                             ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 14:27 PKT ‚îÇ                     ‚îÇ
‚îÇ delete         ‚îÇ                                                             ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 14:33 PKT ‚îÇ 09 Feb 26 14:33 PKT ‚îÇ
‚îÇ start          ‚îÇ --driver=docker --memory=8192 --cpus=4                      ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 14:35 PKT ‚îÇ                     ‚îÇ
‚îÇ start          ‚îÇ --memory=8192 --cpus=4                                      ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 15:53 PKT ‚îÇ                     ‚îÇ
‚îÇ start          ‚îÇ --memory=8192 --cpus=4                                      ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 15:56 PKT ‚îÇ                     ‚îÇ
‚îÇ config         ‚îÇ view                                                        ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 15:56 PKT ‚îÇ 09 Feb 26 15:56 PKT ‚îÇ
‚îÇ start          ‚îÇ --driver=docker --memory=8192 --cpus=4                      ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 15:56 PKT ‚îÇ                     ‚îÇ
‚îÇ start          ‚îÇ --driver=docker --memory=2048 --cpus=2                      ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 15:57 PKT ‚îÇ                     ‚îÇ
‚îÇ start          ‚îÇ --driver=docker --memory=1024mb --cpus=1                    ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 16:00 PKT ‚îÇ                     ‚îÇ
‚îÇ start          ‚îÇ --driver=docker --memory=2048mb --cpus=2 --disk-size=5000mb ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 16:00 PKT ‚îÇ                     ‚îÇ
‚îÇ start          ‚îÇ                                                             ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 16:03 PKT ‚îÇ                     ‚îÇ
‚îÇ delete         ‚îÇ                                                             ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 16:33 PKT ‚îÇ 09 Feb 26 16:34 PKT ‚îÇ
‚îÇ start          ‚îÇ                                                             ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 16:34 PKT ‚îÇ                     ‚îÇ
‚îÇ start          ‚îÇ                                                             ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 16:36 PKT ‚îÇ 09 Feb 26 16:51 PKT ‚îÇ
‚îÇ addons         ‚îÇ enable ingress                                              ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 16:52 PKT ‚îÇ 09 Feb 26 16:56 PKT ‚îÇ
‚îÇ addons         ‚îÇ enable metrics-server                                       ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 16:57 PKT ‚îÇ 09 Feb 26 16:58 PKT ‚îÇ
‚îÇ docker-env     ‚îÇ                                                             ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 16:59 PKT ‚îÇ 09 Feb 26 16:59 PKT ‚îÇ
‚îÇ update-context ‚îÇ                                                             ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 18:21 PKT ‚îÇ 09 Feb 26 18:21 PKT ‚îÇ
‚îÇ start          ‚îÇ                                                             ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 18:21 PKT ‚îÇ                     ‚îÇ
‚îÇ start          ‚îÇ --driver=docker                                             ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 18:22 PKT ‚îÇ                     ‚îÇ
‚îÇ config         ‚îÇ view                                                        ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 18:22 PKT ‚îÇ 09 Feb 26 18:22 PKT ‚îÇ
‚îÇ delete         ‚îÇ                                                             ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 18:23 PKT ‚îÇ 09 Feb 26 18:25 PKT ‚îÇ
‚îÇ start          ‚îÇ --driver=docker                                             ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 18:25 PKT ‚îÇ                     ‚îÇ
‚îÇ start          ‚îÇ --driver=none                                               ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 18:26 PKT ‚îÇ                     ‚îÇ
‚îÇ start          ‚îÇ --driver=docker --alsologtostderr                           ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 18:26 PKT ‚îÇ                     ‚îÇ
‚îÇ start          ‚îÇ --driver=docker --memory=2048mb --cpus=2                    ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 18:29 PKT ‚îÇ                     ‚îÇ
‚îÇ start          ‚îÇ --driver=docker --memory=2048mb --cpus=2                    ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 23:25 PKT ‚îÇ                     ‚îÇ
‚îÇ start          ‚îÇ --driver=docker --memory=2048mb --cpus=2                    ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 23:25 PKT ‚îÇ                     ‚îÇ
‚îÇ start          ‚îÇ                                                             ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 23:26 PKT ‚îÇ                     ‚îÇ
‚îÇ start          ‚îÇ                                                             ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 23:26 PKT ‚îÇ                     ‚îÇ
‚îÇ start          ‚îÇ --driver=docker --memory=2048mb --cpus=2                    ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 23:27 PKT ‚îÇ 09 Feb 26 23:36 PKT ‚îÇ
‚îÇ addons         ‚îÇ enable ingress                                              ‚îÇ minikube ‚îÇ yasirtanoli ‚îÇ v1.38.0 ‚îÇ 09 Feb 26 23:37 PKT ‚îÇ                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò


==> Last Start <==
Log file created at: 2026/02/09 23:27:38
Running on machine: DESKTOP-G89JOMK
Binary: Built with gc go1.25.5 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0209 23:27:38.403511    3152 out.go:360] Setting OutFile to fd 1 ...
I0209 23:27:38.403711    3152 out.go:374] Setting ErrFile to fd 2...
I0209 23:27:38.404038    3152 root.go:338] Updating PATH: /home/yasirtanoli/.minikube/bin
I0209 23:27:38.405240    3152 out.go:368] Setting JSON to false
I0209 23:27:38.406267    3152 start.go:134] hostinfo: {"hostname":"DESKTOP-G89JOMK","uptime":1824,"bootTime":1770659834,"procs":40,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"6.6.114.1-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"guest","hostId":"f95dbe9e-367e-471b-bfd2-6a138448d162"}
I0209 23:27:38.406350    3152 start.go:144] virtualization:  guest
I0209 23:27:38.684706    3152 out.go:179] üòÑ  minikube v1.38.0 on Ubuntu 22.04 (amd64)
I0209 23:27:38.852874    3152 notify.go:220] Checking for updates...
I0209 23:27:38.853447    3152 config.go:183] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.35.0
I0209 23:27:38.854761    3152 driver.go:422] Setting default libvirt URI to qemu:///system
I0209 23:27:39.065092    3152 docker.go:125] docker version: linux-29.2.0:Docker Desktop 4.59.0 (217644)
I0209 23:27:39.065262    3152 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0209 23:27:41.653715    3152 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (2.601096842s)
I0209 23:27:41.654673    3152 info.go:266] docker info: {ID:10bb2f51-055c-4074-97df-292753caba67 Containers:5 ContainersRunning:2 ContainersPaused:0 ContainersStopped:3 Images:4 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:56 OomKillDisable:false NGoroutines:113 SystemTime:2026-02-09 18:27:41.563365283 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:15 KernelVersion:6.6.114.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:2989625344 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///var/run/docker-cli.sock] ExperimentalBuild:false ServerVersion:29.2.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:dea7da592f5d1d2b7755e3a161be07f43fad8f75 Expected:} RuncCommit:{ID:v1.3.4-0-gd6d73eb8 Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/usr/local/lib/docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.17.2] map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-buildx] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.31.1-desktop.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-compose] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v5.0.2] map[Name:debug Path:/usr/local/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.47] map[Name:desktop Path:/usr/local/lib/docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.2.0] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.31] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:/usr/local/lib/docker/cli-plugins/docker-mcp SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.37.0] map[Name:model Path:/usr/local/lib/docker/cli-plugins/docker-model SchemaVersion:0.1.0 ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v1.0.8] map[Name:offload Path:/usr/local/lib/docker/cli-plugins/docker-offload SchemaVersion:0.1.0 ShortDescription:Docker Offload Vendor:Docker Inc. Version:v0.5.41] map[Hidden:true Name:pass Path:/usr/local/lib/docker/cli-plugins/docker-pass SchemaVersion:0.1.0 ShortDescription:Docker Pass Secrets Manager Plugin (beta) Vendor:Docker Inc. Version:v0.0.24] map[Name:sandbox Path:/usr/local/lib/docker/cli-plugins/docker-sandbox SchemaVersion:0.1.0 ShortDescription:Docker Sandbox Vendor:Docker Inc. Version:v0.10.1] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.19.0]] Warnings:<nil>}}
I0209 23:27:41.654884    3152 docker.go:320] overlay module found
I0209 23:27:41.829348    3152 out.go:179] ‚ú®  Using the docker driver based on existing profile
I0209 23:27:41.954654    3152 start.go:310] selected driver: docker
I0209 23:27:41.954757    3152 start.go:932] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 Memory:2851 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.35.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.35.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s Rosetta:false}
I0209 23:27:41.954939    3152 start.go:943] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0209 23:27:42.120829    3152 out.go:203] 
W0209 23:27:42.271537    3152 out.go:285] üßØ  The requested memory allocation of 2048MiB does not leave room for system overhead (total system memory: 2851MiB). You may face stability issues.
W0209 23:27:42.272082    3152 out.go:285] üí°  Suggestion: Start minikube with less memory allocated: 'minikube start --memory=2851mb'
I0209 23:27:42.520520    3152 out.go:203] 
I0209 23:27:42.828100    3152 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0209 23:27:45.503333    3152 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (2.67516111s)
I0209 23:27:45.503739    3152 info.go:266] docker info: {ID:10bb2f51-055c-4074-97df-292753caba67 Containers:5 ContainersRunning:2 ContainersPaused:0 ContainersStopped:3 Images:4 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:56 OomKillDisable:false NGoroutines:113 SystemTime:2026-02-09 18:27:45.463882199 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:15 KernelVersion:6.6.114.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:2989625344 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///var/run/docker-cli.sock] ExperimentalBuild:false ServerVersion:29.2.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:dea7da592f5d1d2b7755e3a161be07f43fad8f75 Expected:} RuncCommit:{ID:v1.3.4-0-gd6d73eb8 Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/usr/local/lib/docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.17.2] map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-buildx] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.31.1-desktop.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-compose] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v5.0.2] map[Name:debug Path:/usr/local/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.47] map[Name:desktop Path:/usr/local/lib/docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.2.0] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.31] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:/usr/local/lib/docker/cli-plugins/docker-mcp SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.37.0] map[Name:model Path:/usr/local/lib/docker/cli-plugins/docker-model SchemaVersion:0.1.0 ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v1.0.8] map[Name:offload Path:/usr/local/lib/docker/cli-plugins/docker-offload SchemaVersion:0.1.0 ShortDescription:Docker Offload Vendor:Docker Inc. Version:v0.5.41] map[Hidden:true Name:pass Path:/usr/local/lib/docker/cli-plugins/docker-pass SchemaVersion:0.1.0 ShortDescription:Docker Pass Secrets Manager Plugin (beta) Vendor:Docker Inc. Version:v0.0.24] map[Name:sandbox Path:/usr/local/lib/docker/cli-plugins/docker-sandbox SchemaVersion:0.1.0 ShortDescription:Docker Sandbox Vendor:Docker Inc. Version:v0.10.1] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.19.0]] Warnings:<nil>}}
I0209 23:27:45.684648    3152 out.go:203] 
W0209 23:27:45.801737    3152 out.go:285] üßØ  The requested memory allocation of 2048MiB does not leave room for system overhead (total system memory: 2851MiB). You may face stability issues.
W0209 23:27:45.802216    3152 out.go:285] üí°  Suggestion: Start minikube with less memory allocated: 'minikube start --memory=2851mb'
I0209 23:27:45.926311    3152 out.go:203] 
W0209 23:27:46.076202    3152 out.go:285] ‚ùó  You cannot change the memory size for an existing minikube cluster. Please first delete the cluster.
I0209 23:27:46.201227    3152 out.go:203] 
W0209 23:27:46.342857    3152 out.go:285] üßØ  The requested memory allocation of 2851MiB does not leave room for system overhead (total system memory: 2851MiB). You may face stability issues.
W0209 23:27:46.343061    3152 out.go:285] üí°  Suggestion: Start minikube with less memory allocated: 'minikube start --memory=2851mb'
I0209 23:27:46.475797    3152 out.go:203] 
I0209 23:27:46.775704    3152 cni.go:83] Creating CNI manager for ""
I0209 23:27:46.804433    3152 cni.go:157] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0209 23:27:46.809939    3152 start.go:357] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 Memory:2851 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.35.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.35.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s Rosetta:false}
I0209 23:27:47.333648    3152 out.go:179] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0209 23:27:47.492268    3152 cache.go:135] Beginning downloading kic base image for docker with docker
I0209 23:27:47.633978    3152 out.go:179] üöú  Pulling base image v0.0.49 ...
I0209 23:27:47.758481    3152 image.go:82] Checking for gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 in local docker daemon
I0209 23:27:47.758534    3152 preload.go:187] Checking if preload exists for k8s version v1.35.0 and runtime docker
I0209 23:27:47.758708    3152 preload.go:202] Found local preload: /home/yasirtanoli/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.35.0-docker-overlay2-amd64.tar.lz4
I0209 23:27:47.758754    3152 cache.go:66] Caching tarball of preloaded images
I0209 23:27:48.049969    3152 out.go:252] ‚åõ  Another minikube instance is downloading dependencies... 
I0209 23:27:48.050133    3152 preload.go:250] Found /home/yasirtanoli/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.35.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0209 23:27:48.050172    3152 cache.go:69] Finished verifying existence of preloaded tar for v1.35.0 on docker
I0209 23:27:48.135714    3152 profile.go:143] Saving config to /home/yasirtanoli/.minikube/profiles/minikube/config.json ...
I0209 23:27:48.245197    3152 cache.go:164] Downloading gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 to local cache
I0209 23:27:48.331104    3152 image.go:66] Checking for gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 in local cache directory
I0209 23:27:48.519721    3152 image.go:69] Found gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 in local cache directory, skipping pull
I0209 23:27:48.519736    3152 image.go:138] gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 exists in cache, skipping pull
I0209 23:27:48.519768    3152 cache.go:167] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 as a tarball
I0209 23:27:48.519773    3152 cache.go:177] Loading gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 from local cache
I0209 23:27:48.696322    3152 cache.go:183] failed to download gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945, will try fallback image if available: tarball: unexpected EOF
I0209 23:27:48.696341    3152 image.go:82] Checking for docker.io/kicbase/stable:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 in local docker daemon
I0209 23:27:49.521874    3152 image.go:101] Found docker.io/kicbase/stable:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 in local docker daemon, skipping pull
I0209 23:27:49.521895    3152 cache.go:159] docker.io/kicbase/stable:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 exists in daemon, skipping load
W0209 23:27:49.521928    3152 out.go:285] ‚ùó  minikube was unable to download gcr.io/k8s-minikube/kicbase:v0.0.49, but successfully downloaded docker.io/kicbase/stable:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 as a fallback image
I0209 23:27:49.522072    3152 cache.go:244] Successfully downloaded all kic artifacts
I0209 23:27:49.522134    3152 start.go:359] acquireMachinesLock for minikube: {Name:mkf18adbc2a1363284c0196a26d11e62a8eef735 Timeout:10m0s Delay:500ms}
I0209 23:27:49.729409    3152 start.go:363] duration metric: took 207.236612ms to acquireMachinesLock for "minikube"
I0209 23:27:49.729476    3152 start.go:95] Skipping create...Using existing machine configuration
I0209 23:27:49.729490    3152 fix.go:53] fixHost starting: 
I0209 23:27:49.730065    3152 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0209 23:27:50.255775    3152 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0209 23:27:50.255854    3152 fix.go:111] recreateIfNeeded on minikube: state= err=unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0209 23:27:50.255868    3152 fix.go:116] machineExists: false. err=machine does not exist
I0209 23:27:50.472577    3152 out.go:179] ü§∑  docker "minikube" container is missing, will recreate.
I0209 23:27:50.631200    3152 delete.go:124] DEMOLISHING minikube ...
I0209 23:27:50.631955    3152 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0209 23:27:50.718806    3152 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
W0209 23:27:50.718836    3152 stop.go:83] unable to get state: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0209 23:27:50.718847    3152 delete.go:128] stophost failed (probably ok): ssh power off: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0209 23:27:50.719388    3152 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0209 23:27:50.783491    3152 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0209 23:27:50.783534    3152 delete.go:82] Unable to get host status for minikube, assuming it has already been deleted: state: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0209 23:27:50.861540    3152 cli_runner.go:164] Run: docker container inspect -f {{.Id}} minikube
W0209 23:27:50.918402    3152 cli_runner.go:211] docker container inspect -f {{.Id}} minikube returned with exit code 1
I0209 23:27:50.918484    3152 kic.go:370] could not find the container minikube to remove it. will try anyways
I0209 23:27:50.918618    3152 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0209 23:27:51.008236    3152 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
W0209 23:27:51.008407    3152 oci.go:83] error getting container status, will try to delete anyways: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0209 23:27:51.008488    3152 cli_runner.go:164] Run: docker exec --privileged -t minikube /bin/bash -c "sudo init 0"
W0209 23:27:51.065560    3152 cli_runner.go:211] docker exec --privileged -t minikube /bin/bash -c "sudo init 0" returned with exit code 1
I0209 23:27:51.065610    3152 oci.go:658] error shutdown minikube: docker exec --privileged -t minikube /bin/bash -c "sudo init 0": exit status 1
stdout:

stderr:
Error response from daemon: No such container: minikube
I0209 23:27:52.066540    3152 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0209 23:27:52.127133    3152 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0209 23:27:52.127166    3152 oci.go:670] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0209 23:27:52.127176    3152 oci.go:672] temporary error: container minikube status is  but expect it to be exited
I0209 23:27:52.196260    3152 retry.go:85] will retry after 300ms: couldn't verify container is exited: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0209 23:27:52.468376    3152 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0209 23:27:52.524384    3152 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0209 23:27:52.524425    3152 oci.go:670] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0209 23:27:52.524432    3152 oci.go:672] temporary error: container minikube status is  but expect it to be exited
I0209 23:27:53.016453    3152 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0209 23:27:53.138985    3152 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0209 23:27:53.139024    3152 oci.go:670] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0209 23:27:53.139032    3152 oci.go:672] temporary error: container minikube status is  but expect it to be exited
I0209 23:27:53.750630    3152 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0209 23:27:53.834397    3152 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0209 23:27:53.834431    3152 oci.go:670] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0209 23:27:53.834438    3152 oci.go:672] temporary error: container minikube status is  but expect it to be exited
I0209 23:27:55.806472    3152 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0209 23:27:55.928594    3152 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0209 23:27:55.928638    3152 oci.go:670] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0209 23:27:55.928645    3152 oci.go:672] temporary error: container minikube status is  but expect it to be exited
I0209 23:27:57.600875    3152 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0209 23:27:58.735777    3152 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0209 23:27:58.735801    3152 cli_runner.go:217] Completed: docker container inspect minikube --format={{.State.Status}}: (1.134823991s)
I0209 23:27:58.735858    3152 oci.go:670] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0209 23:27:58.735865    3152 oci.go:672] temporary error: container minikube status is  but expect it to be exited
W0209 23:27:59.084441    3152 notify.go:59] Error getting json from minikube version url: error with http GET for endpoint https://storage.googleapis.com/minikube/releases-v2.json: Get "https://storage.googleapis.com/minikube/releases-v2.json": dial tcp: lookup storage.googleapis.com on 172.20.240.1:53: read udp 172.20.253.224:36721->172.20.240.1:53: i/o timeout
I0209 23:28:00.997544    3152 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0209 23:28:01.119454    3152 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0209 23:28:01.119498    3152 oci.go:670] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0209 23:28:01.119506    3152 oci.go:672] temporary error: container minikube status is  but expect it to be exited
I0209 23:28:07.101379    3152 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0209 23:28:07.181626    3152 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0209 23:28:07.181672    3152 oci.go:670] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0209 23:28:07.181681    3152 oci.go:672] temporary error: container minikube status is  but expect it to be exited
I0209 23:28:07.181758    3152 oci.go:87] couldn't shut down minikube (might be okay): verify shutdown: couldn't verify container is exited: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
 
I0209 23:28:07.181881    3152 cli_runner.go:164] Run: docker rm -f -v minikube
I0209 23:28:07.753831    3152 cli_runner.go:164] Run: docker container inspect -f {{.Id}} minikube
W0209 23:28:07.813550    3152 cli_runner.go:211] docker container inspect -f {{.Id}} minikube returned with exit code 1
I0209 23:28:07.813738    3152 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0209 23:28:08.281671    3152 cli_runner.go:164] Run: docker network rm minikube
I0209 23:28:09.474877    3152 cli_runner.go:217] Completed: docker network rm minikube: (1.193129505s)
I0209 23:28:09.475701    3152 fix.go:123] Sleeping 1 second for extra luck!
I0209 23:28:10.476052    3152 start.go:124] createHost starting for "" (driver="docker")
I0209 23:28:10.773259    3152 out.go:252] üî•  Creating docker container (CPUs=2, Memory=2851MB) ...
I0209 23:28:10.774025    3152 start.go:158] libmachine.API.Create for "minikube" (driver="docker")
I0209 23:28:10.774195    3152 client.go:173] LocalClient.Create starting
I0209 23:28:10.815832    3152 main.go:144] libmachine: Reading certificate data from /home/yasirtanoli/.minikube/certs/ca.pem
I0209 23:28:10.823204    3152 main.go:144] libmachine: Decoding PEM data...
I0209 23:28:10.823249    3152 main.go:144] libmachine: Parsing certificate...
I0209 23:28:10.864994    3152 main.go:144] libmachine: Reading certificate data from /home/yasirtanoli/.minikube/certs/cert.pem
I0209 23:28:10.874218    3152 main.go:144] libmachine: Decoding PEM data...
I0209 23:28:10.874246    3152 main.go:144] libmachine: Parsing certificate...
I0209 23:28:10.875105    3152 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0209 23:28:10.939750    3152 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0209 23:28:10.939904    3152 network_create.go:285] running [docker network inspect minikube] to gather additional debugging logs...
I0209 23:28:10.939926    3152 cli_runner.go:164] Run: docker network inspect minikube
W0209 23:28:10.989884    3152 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0209 23:28:10.989918    3152 network_create.go:288] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0209 23:28:10.989961    3152 network_create.go:290] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0209 23:28:10.990185    3152 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0209 23:28:11.046276    3152 network.go:205] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc00080c3c8}
I0209 23:28:11.046321    3152 network_create.go:125] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0209 23:28:11.046445    3152 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0209 23:28:11.519195    3152 network_create.go:109] docker network minikube 192.168.49.0/24 created
I0209 23:28:11.519226    3152 kic.go:120] calculated static IP "192.168.49.2" for the "minikube" container
I0209 23:28:11.519373    3152 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0209 23:28:11.804477    3152 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0209 23:28:12.578355    3152 oci.go:102] Successfully created a docker volume minikube
I0209 23:28:12.578563    3152 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var docker.io/kicbase/stable:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 -d /var/lib
I0209 23:28:38.240282    3152 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var docker.io/kicbase/stable:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 -d /var/lib: (25.66163891s)
I0209 23:28:38.240334    3152 oci.go:106] Successfully prepared a docker volume minikube
I0209 23:28:38.304562    3152 preload.go:187] Checking if preload exists for k8s version v1.35.0 and runtime docker
I0209 23:28:38.304593    3152 kic.go:193] Starting extracting preloaded images to volume ...
I0209 23:28:38.304807    3152 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/yasirtanoli/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.35.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir docker.io/kicbase/stable:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 -I lz4 -xf /preloaded.tar -C /extractDir
I0209 23:29:35.264870    3152 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/yasirtanoli/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.35.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir docker.io/kicbase/stable:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 -I lz4 -xf /preloaded.tar -C /extractDir: (56.976025629s)
I0209 23:29:35.265192    3152 kic.go:202] duration metric: took 56.9766476s to extract preloaded images to volume ...
W0209 23:29:35.265567    3152 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
W0209 23:29:35.266759    3152 oci.go:251] Your kernel does not support CPU cfs period/quota or the cgroup is not mounted.
I0209 23:29:35.267023    3152 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0209 23:29:59.718255    3152 cli_runner.go:217] Completed: docker info --format "'{{json .SecurityOptions}}'": (24.457326426s)
I0209 23:29:59.755690    3152 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2851mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 docker.io/kicbase/stable:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945
I0209 23:30:08.111036    3152 cli_runner.go:217] Completed: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2851mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 docker.io/kicbase/stable:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945: (8.355176897s)
I0209 23:30:08.213819    3152 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0209 23:30:08.364177    3152 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0209 23:30:08.585845    3152 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0209 23:30:09.207247    3152 oci.go:143] the created container "minikube" has a running status.
I0209 23:30:09.213940    3152 kic.go:224] Creating ssh key for kic: /home/yasirtanoli/.minikube/machines/minikube/id_rsa...
I0209 23:30:10.201791    3152 kic_runner.go:190] docker (temp): /home/yasirtanoli/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0209 23:30:11.142089    3152 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0209 23:30:11.229202    3152 kic_runner.go:92] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0209 23:30:11.229219    3152 kic_runner.go:113] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0209 23:30:11.538034    3152 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0209 23:30:11.951896    3152 machine.go:96] provisionDockerMachine start ...
I0209 23:30:12.068167    3152 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0209 23:30:12.562650    3152 main.go:144] libmachine: Using SSH client type: native
I0209 23:30:13.211666    3152 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 63522 <nil> <nil>}
I0209 23:30:13.211719    3152 main.go:144] libmachine: About to run SSH command:
hostname
I0209 23:30:13.588033    3152 main.go:144] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:45558->127.0.0.1:63522: read: connection reset by peer
I0209 23:30:18.908670    3152 main.go:144] libmachine: SSH cmd err, output: <nil>: minikube

I0209 23:30:18.943907    3152 ubuntu.go:182] provisioning hostname "minikube"
I0209 23:30:18.968724    3152 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0209 23:30:19.037448    3152 main.go:144] libmachine: Using SSH client type: native
I0209 23:30:19.037875    3152 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 63522 <nil> <nil>}
I0209 23:30:19.037894    3152 main.go:144] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0209 23:30:20.291384    3152 main.go:144] libmachine: SSH cmd err, output: <nil>: minikube

I0209 23:30:20.348081    3152 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0209 23:30:20.847040    3152 main.go:144] libmachine: Using SSH client type: native
I0209 23:30:20.847800    3152 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 63522 <nil> <nil>}
I0209 23:30:20.847838    3152 main.go:144] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0209 23:30:21.111303    3152 main.go:144] libmachine: SSH cmd err, output: <nil>: 
I0209 23:30:21.132758    3152 ubuntu.go:188] set auth options {CertDir:/home/yasirtanoli/.minikube CaCertPath:/home/yasirtanoli/.minikube/certs/ca.pem CaPrivateKeyPath:/home/yasirtanoli/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/yasirtanoli/.minikube/machines/server.pem ServerKeyPath:/home/yasirtanoli/.minikube/machines/server-key.pem ClientKeyPath:/home/yasirtanoli/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/yasirtanoli/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/yasirtanoli/.minikube}
I0209 23:30:21.132810    3152 ubuntu.go:190] setting up certificates
I0209 23:30:21.132821    3152 provision.go:83] configureAuth start
I0209 23:30:21.132917    3152 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0209 23:30:21.568125    3152 provision.go:142] copyHostCerts
I0209 23:30:21.664548    3152 exec_runner.go:143] found /home/yasirtanoli/.minikube/ca.pem, removing ...
I0209 23:30:21.832695    3152 exec_runner.go:202] rm: /home/yasirtanoli/.minikube/ca.pem
I0209 23:30:21.990090    3152 exec_runner.go:150] cp: /home/yasirtanoli/.minikube/certs/ca.pem --> /home/yasirtanoli/.minikube/ca.pem (1090 bytes)
I0209 23:30:22.075858    3152 exec_runner.go:143] found /home/yasirtanoli/.minikube/cert.pem, removing ...
I0209 23:30:22.075882    3152 exec_runner.go:202] rm: /home/yasirtanoli/.minikube/cert.pem
I0209 23:30:22.075994    3152 exec_runner.go:150] cp: /home/yasirtanoli/.minikube/certs/cert.pem --> /home/yasirtanoli/.minikube/cert.pem (1135 bytes)
I0209 23:30:22.185213    3152 exec_runner.go:143] found /home/yasirtanoli/.minikube/key.pem, removing ...
I0209 23:30:22.185239    3152 exec_runner.go:202] rm: /home/yasirtanoli/.minikube/key.pem
I0209 23:30:22.185347    3152 exec_runner.go:150] cp: /home/yasirtanoli/.minikube/certs/key.pem --> /home/yasirtanoli/.minikube/key.pem (1679 bytes)
I0209 23:30:22.425015    3152 provision.go:116] generating server cert: /home/yasirtanoli/.minikube/machines/server.pem ca-key=/home/yasirtanoli/.minikube/certs/ca.pem private-key=/home/yasirtanoli/.minikube/certs/ca-key.pem org=yasirtanoli.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0209 23:30:22.841200    3152 provision.go:176] copyRemoteCerts
I0209 23:30:23.151618    3152 ssh_runner.go:194] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0209 23:30:23.151866    3152 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0209 23:30:23.227527    3152 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63522 SSHKeyPath:/home/yasirtanoli/.minikube/machines/minikube/id_rsa Username:docker}
I0209 23:30:23.590163    3152 ssh_runner.go:361] scp /home/yasirtanoli/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0209 23:30:24.202403    3152 ssh_runner.go:361] scp /home/yasirtanoli/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1090 bytes)
I0209 23:30:24.247322    3152 ssh_runner.go:361] scp /home/yasirtanoli/.minikube/machines/server.pem --> /etc/docker/server.pem (1192 bytes)
I0209 23:30:24.411504    3152 provision.go:86] duration metric: took 3.154755605s to configureAuth
I0209 23:30:24.411547    3152 ubuntu.go:206] setting minikube options for container-runtime
I0209 23:30:24.514349    3152 config.go:183] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.35.0
I0209 23:30:24.551025    3152 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0209 23:30:24.680473    3152 main.go:144] libmachine: Using SSH client type: native
I0209 23:30:24.680782    3152 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 63522 <nil> <nil>}
I0209 23:30:24.680790    3152 main.go:144] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0209 23:30:24.892024    3152 main.go:144] libmachine: SSH cmd err, output: <nil>: overlay

I0209 23:30:24.919813    3152 ubuntu.go:71] root file system type: overlay
I0209 23:30:24.983177    3152 provision.go:313] Updating docker unit: /lib/systemd/system/docker.service ...
I0209 23:30:24.983371    3152 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0209 23:30:25.047609    3152 main.go:144] libmachine: Using SSH client type: native
I0209 23:30:25.047909    3152 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 63522 <nil> <nil>}
I0209 23:30:25.048026    3152 main.go:144] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0209 23:30:25.279378    3152 main.go:144] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I0209 23:30:25.310662    3152 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0209 23:30:25.366881    3152 main.go:144] libmachine: Using SSH client type: native
I0209 23:30:25.367217    3152 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 63522 <nil> <nil>}
I0209 23:30:25.367233    3152 main.go:144] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0209 23:30:45.896937    3152 main.go:144] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2026-01-26 19:24:35.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2026-02-09 18:30:25.272867332 +0000
@@ -9,23 +9,34 @@
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
 Restart=always
 
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
+
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0209 23:30:45.896997    3152 machine.go:99] duration metric: took 33.950343198s to provisionDockerMachine
I0209 23:30:45.897047    3152 client.go:176] duration metric: took 2m35.162760483s to LocalClient.Create
I0209 23:30:45.897093    3152 start.go:166] duration metric: took 2m35.163039205s to libmachine.API.Create "minikube"
I0209 23:30:45.897110    3152 start.go:292] postStartSetup for "minikube" (driver="docker")
I0209 23:30:45.897158    3152 start.go:321] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0209 23:30:45.897427    3152 ssh_runner.go:194] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0209 23:30:45.897536    3152 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0209 23:30:45.999916    3152 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63522 SSHKeyPath:/home/yasirtanoli/.minikube/machines/minikube/id_rsa Username:docker}
I0209 23:30:46.345552    3152 ssh_runner.go:194] Run: cat /etc/os-release
I0209 23:30:46.373462    3152 main.go:144] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0209 23:30:46.373544    3152 info.go:137] Remote host: Debian GNU/Linux 12 (bookworm)
I0209 23:30:46.373648    3152 filesync.go:125] Scanning /home/yasirtanoli/.minikube/addons for local assets ...
I0209 23:30:46.439634    3152 filesync.go:125] Scanning /home/yasirtanoli/.minikube/files for local assets ...
I0209 23:30:46.465604    3152 start.go:295] duration metric: took 568.447471ms for postStartSetup
I0209 23:30:46.485847    3152 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0209 23:30:46.556689    3152 profile.go:143] Saving config to /home/yasirtanoli/.minikube/profiles/minikube/config.json ...
I0209 23:30:46.557161    3152 ssh_runner.go:194] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0209 23:30:46.557266    3152 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0209 23:30:46.635375    3152 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63522 SSHKeyPath:/home/yasirtanoli/.minikube/machines/minikube/id_rsa Username:docker}
I0209 23:30:46.787737    3152 ssh_runner.go:194] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0209 23:30:46.799367    3152 start.go:127] duration metric: took 2m36.363191428s to createHost
I0209 23:30:46.799578    3152 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0209 23:30:46.907963    3152 machine.go:96] provisionDockerMachine start ...
I0209 23:30:46.908134    3152 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0209 23:30:47.002172    3152 main.go:144] libmachine: Using SSH client type: native
I0209 23:30:47.002562    3152 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 63522 <nil> <nil>}
I0209 23:30:47.002572    3152 main.go:144] libmachine: About to run SSH command:
hostname
I0209 23:30:47.244307    3152 main.go:144] libmachine: SSH cmd err, output: <nil>: minikube

I0209 23:30:47.244335    3152 ubuntu.go:182] provisioning hostname "minikube"
I0209 23:30:47.244461    3152 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0209 23:30:47.330888    3152 main.go:144] libmachine: Using SSH client type: native
I0209 23:30:47.331380    3152 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 63522 <nil> <nil>}
I0209 23:30:47.331395    3152 main.go:144] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0209 23:30:47.558409    3152 main.go:144] libmachine: SSH cmd err, output: <nil>: minikube

I0209 23:30:47.558602    3152 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0209 23:30:47.650183    3152 main.go:144] libmachine: Using SSH client type: native
I0209 23:30:47.650487    3152 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 63522 <nil> <nil>}
I0209 23:30:47.650510    3152 main.go:144] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0209 23:30:47.821043    3152 main.go:144] libmachine: SSH cmd err, output: <nil>: 
I0209 23:30:47.821072    3152 ubuntu.go:188] set auth options {CertDir:/home/yasirtanoli/.minikube CaCertPath:/home/yasirtanoli/.minikube/certs/ca.pem CaPrivateKeyPath:/home/yasirtanoli/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/yasirtanoli/.minikube/machines/server.pem ServerKeyPath:/home/yasirtanoli/.minikube/machines/server-key.pem ClientKeyPath:/home/yasirtanoli/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/yasirtanoli/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/yasirtanoli/.minikube}
I0209 23:30:47.821111    3152 ubuntu.go:190] setting up certificates
I0209 23:30:47.821127    3152 provision.go:83] configureAuth start
I0209 23:30:47.821285    3152 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0209 23:30:47.875682    3152 provision.go:142] copyHostCerts
I0209 23:30:47.875757    3152 exec_runner.go:143] found /home/yasirtanoli/.minikube/ca.pem, removing ...
I0209 23:30:47.875766    3152 exec_runner.go:202] rm: /home/yasirtanoli/.minikube/ca.pem
I0209 23:30:47.875872    3152 exec_runner.go:150] cp: /home/yasirtanoli/.minikube/certs/ca.pem --> /home/yasirtanoli/.minikube/ca.pem (1090 bytes)
I0209 23:30:47.876044    3152 exec_runner.go:143] found /home/yasirtanoli/.minikube/cert.pem, removing ...
I0209 23:30:47.876049    3152 exec_runner.go:202] rm: /home/yasirtanoli/.minikube/cert.pem
I0209 23:30:47.876089    3152 exec_runner.go:150] cp: /home/yasirtanoli/.minikube/certs/cert.pem --> /home/yasirtanoli/.minikube/cert.pem (1135 bytes)
I0209 23:30:47.876216    3152 exec_runner.go:143] found /home/yasirtanoli/.minikube/key.pem, removing ...
I0209 23:30:47.876221    3152 exec_runner.go:202] rm: /home/yasirtanoli/.minikube/key.pem
I0209 23:30:47.876264    3152 exec_runner.go:150] cp: /home/yasirtanoli/.minikube/certs/key.pem --> /home/yasirtanoli/.minikube/key.pem (1679 bytes)
I0209 23:30:47.876363    3152 provision.go:116] generating server cert: /home/yasirtanoli/.minikube/machines/server.pem ca-key=/home/yasirtanoli/.minikube/certs/ca.pem private-key=/home/yasirtanoli/.minikube/certs/ca-key.pem org=yasirtanoli.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0209 23:30:47.913352    3152 provision.go:176] copyRemoteCerts
I0209 23:30:47.913444    3152 ssh_runner.go:194] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0209 23:30:47.913500    3152 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0209 23:30:48.002412    3152 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63522 SSHKeyPath:/home/yasirtanoli/.minikube/machines/minikube/id_rsa Username:docker}
I0209 23:30:48.125080    3152 ssh_runner.go:361] scp /home/yasirtanoli/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1090 bytes)
I0209 23:30:48.162325    3152 ssh_runner.go:361] scp /home/yasirtanoli/.minikube/machines/server.pem --> /etc/docker/server.pem (1192 bytes)
I0209 23:30:48.203768    3152 ssh_runner.go:361] scp /home/yasirtanoli/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0209 23:30:48.235892    3152 provision.go:86] duration metric: took 414.744567ms to configureAuth
I0209 23:30:48.235920    3152 ubuntu.go:206] setting minikube options for container-runtime
I0209 23:30:48.236175    3152 config.go:183] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.35.0
I0209 23:30:48.236261    3152 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0209 23:30:48.292034    3152 main.go:144] libmachine: Using SSH client type: native
I0209 23:30:48.292312    3152 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 63522 <nil> <nil>}
I0209 23:30:48.292321    3152 main.go:144] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0209 23:30:48.460888    3152 main.go:144] libmachine: SSH cmd err, output: <nil>: overlay

I0209 23:30:48.460905    3152 ubuntu.go:71] root file system type: overlay
I0209 23:30:48.461154    3152 provision.go:313] Updating docker unit: /lib/systemd/system/docker.service ...
I0209 23:30:48.461337    3152 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0209 23:30:48.517549    3152 main.go:144] libmachine: Using SSH client type: native
I0209 23:30:48.517917    3152 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 63522 <nil> <nil>}
I0209 23:30:48.517993    3152 main.go:144] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0209 23:30:48.703741    3152 main.go:144] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I0209 23:30:48.703974    3152 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0209 23:30:48.761722    3152 main.go:144] libmachine: Using SSH client type: native
I0209 23:30:48.762043    3152 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 63522 <nil> <nil>}
I0209 23:30:48.762058    3152 main.go:144] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0209 23:30:48.936272    3152 main.go:144] libmachine: SSH cmd err, output: <nil>: 
I0209 23:30:48.936289    3152 machine.go:99] duration metric: took 2.0283118s to provisionDockerMachine
I0209 23:30:48.936301    3152 start.go:292] postStartSetup for "minikube" (driver="docker")
I0209 23:30:48.936313    3152 start.go:321] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0209 23:30:48.936422    3152 ssh_runner.go:194] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0209 23:30:48.936503    3152 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0209 23:30:48.991364    3152 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63522 SSHKeyPath:/home/yasirtanoli/.minikube/machines/minikube/id_rsa Username:docker}
I0209 23:30:49.125721    3152 ssh_runner.go:194] Run: cat /etc/os-release
I0209 23:30:49.135838    3152 main.go:144] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0209 23:30:49.135916    3152 info.go:137] Remote host: Debian GNU/Linux 12 (bookworm)
I0209 23:30:49.135935    3152 filesync.go:125] Scanning /home/yasirtanoli/.minikube/addons for local assets ...
I0209 23:30:49.136052    3152 filesync.go:125] Scanning /home/yasirtanoli/.minikube/files for local assets ...
I0209 23:30:49.136122    3152 start.go:295] duration metric: took 199.810818ms for postStartSetup
I0209 23:30:49.136257    3152 ssh_runner.go:194] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0209 23:30:49.136443    3152 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0209 23:30:49.218786    3152 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63522 SSHKeyPath:/home/yasirtanoli/.minikube/machines/minikube/id_rsa Username:docker}
I0209 23:30:49.356062    3152 ssh_runner.go:194] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0209 23:30:49.367220    3152 fix.go:55] duration metric: took 2m59.677649403s for fixHost
I0209 23:30:49.367254    3152 start.go:82] releasing machines lock for "minikube", held for 2m59.677739229s
I0209 23:30:49.367447    3152 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0209 23:30:49.466762    3152 ssh_runner.go:194] Run: cat /version.json
I0209 23:30:49.466884    3152 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0209 23:30:49.513176    3152 ssh_runner.go:194] Run: curl -sS -m 2 https://registry.k8s.io/
I0209 23:30:49.528514    3152 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0209 23:30:49.548823    3152 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63522 SSHKeyPath:/home/yasirtanoli/.minikube/machines/minikube/id_rsa Username:docker}
I0209 23:30:49.624701    3152 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63522 SSHKeyPath:/home/yasirtanoli/.minikube/machines/minikube/id_rsa Username:docker}
I0209 23:30:49.808735    3152 ssh_runner.go:194] Run: systemctl --version
I0209 23:30:51.601485    3152 ssh_runner.go:234] Completed: curl -sS -m 2 https://registry.k8s.io/: (2.08826302s)
I0209 23:30:51.601585    3152 ssh_runner.go:234] Completed: systemctl --version: (1.792824436s)
I0209 23:30:51.601749    3152 ssh_runner.go:194] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
W0209 23:30:51.614839    3152 start.go:881] [curl -sS -m 2 https://registry.k8s.io/] failed: curl -sS -m 2 https://registry.k8s.io/: Process exited with status 6
stdout:

stderr:
curl: (6) Could not resolve host: registry.k8s.io
W0209 23:30:51.749384    3152 cni.go:208] loopback cni configuration skipped: "/etc/cni/net.d/*loopback.conf*" not found
I0209 23:30:51.749558    3152 ssh_runner.go:194] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0209 23:30:52.301700    3152 cni.go:261] disabled [/etc/cni/net.d/87-podman-bridge.conflist, /etc/cni/net.d/10-crio-bridge.conflist.disabled] bridge cni config(s)
I0209 23:30:52.301741    3152 start.go:497] detecting cgroup driver to use...
I0209 23:30:52.301832    3152 detect.go:178] detected "systemd" cgroup driver on host os
I0209 23:30:52.302248    3152 ssh_runner.go:194] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0209 23:30:52.344848    3152 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10.1"|' /etc/containerd/config.toml"
I0209 23:30:52.364258    3152 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0209 23:30:52.381587    3152 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0209 23:30:52.381753    3152 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0209 23:30:52.399501    3152 ssh_runner.go:194] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0209 23:30:52.415913    3152 ssh_runner.go:194] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0209 23:30:52.433456    3152 ssh_runner.go:194] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0209 23:30:52.458932    3152 ssh_runner.go:194] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0209 23:30:52.540734    3152 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0209 23:30:52.562262    3152 ssh_runner.go:194] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0209 23:30:52.670420    3152 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0209 23:30:52.688431    3152 ssh_runner.go:194] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0209 23:30:52.717506    3152 ssh_runner.go:194] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0209 23:30:52.819545    3152 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0209 23:30:53.216296    3152 ssh_runner.go:194] Run: sudo systemctl restart containerd
I0209 23:30:54.400365    3152 ssh_runner.go:234] Completed: sudo systemctl restart containerd: (1.184016671s)
I0209 23:30:54.400427    3152 start.go:497] detecting cgroup driver to use...
I0209 23:30:54.400494    3152 detect.go:178] detected "systemd" cgroup driver on host os
I0209 23:30:54.400623    3152 ssh_runner.go:194] Run: sudo systemctl cat docker.service
I0209 23:30:54.427128    3152 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service containerd
I0209 23:30:54.450443    3152 ssh_runner.go:194] Run: sudo systemctl stop -f containerd
I0209 23:30:54.537935    3152 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service containerd
I0209 23:30:54.563444    3152 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service crio
I0209 23:30:54.616238    3152 ssh_runner.go:194] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0209 23:30:54.644850    3152 ssh_runner.go:194] Run: which cri-dockerd
I0209 23:30:54.652063    3152 ssh_runner.go:194] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0209 23:30:54.667335    3152 ssh_runner.go:361] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (192 bytes)
I0209 23:30:54.691571    3152 ssh_runner.go:194] Run: sudo systemctl unmask docker.service
I0209 23:30:54.946478    3152 ssh_runner.go:194] Run: sudo systemctl enable docker.socket
I0209 23:30:55.128271    3152 docker.go:577] configuring docker to use "systemd" as cgroup driver...
I0209 23:30:55.240563    3152 ssh_runner.go:361] scp memory --> /etc/docker/daemon.json (129 bytes)
I0209 23:30:55.290481    3152 ssh_runner.go:194] Run: sudo systemctl reset-failed docker
I0209 23:30:55.317649    3152 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0209 23:30:55.495801    3152 ssh_runner.go:194] Run: sudo systemctl restart docker
I0209 23:30:59.690136    3152 ssh_runner.go:234] Completed: sudo systemctl restart docker: (4.194300079s)
I0209 23:30:59.690275    3152 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service docker
I0209 23:30:59.711348    3152 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0209 23:30:59.763664    3152 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service cri-docker.service
I0209 23:30:59.786370    3152 ssh_runner.go:194] Run: sudo systemctl unmask cri-docker.socket
I0209 23:30:59.992733    3152 ssh_runner.go:194] Run: sudo systemctl enable cri-docker.socket
I0209 23:31:00.160339    3152 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0209 23:31:00.348687    3152 ssh_runner.go:194] Run: sudo systemctl restart cri-docker.socket
I0209 23:31:00.401492    3152 ssh_runner.go:194] Run: sudo systemctl reset-failed cri-docker.service
I0209 23:31:00.421654    3152 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0209 23:31:00.602317    3152 ssh_runner.go:194] Run: sudo systemctl restart cri-docker.service
W0209 23:31:04.151723    3152 out.go:285] ‚ùó  Failing to connect to https://registry.k8s.io/ from both inside the minikube container and host machine
W0209 23:31:04.151866    3152 out.go:285] üí°  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0209 23:31:05.590661    3152 ssh_runner.go:234] Completed: sudo systemctl restart cri-docker.service: (4.98830886s)
I0209 23:31:05.590859    3152 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service cri-docker.service
I0209 23:31:05.710104    3152 start.go:554] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0209 23:31:05.710280    3152 ssh_runner.go:194] Run: stat /var/run/cri-dockerd.sock
I0209 23:31:05.719355    3152 start.go:575] Will wait 60s for crictl version
I0209 23:31:05.719522    3152 ssh_runner.go:194] Run: which crictl
I0209 23:31:05.730278    3152 ssh_runner.go:194] Run: sudo /usr/local/bin/crictl version
I0209 23:31:09.572313    3152 ssh_runner.go:234] Completed: sudo /usr/local/bin/crictl version: (3.841999095s)
I0209 23:31:09.572379    3152 start.go:591] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  29.2.0
RuntimeApiVersion:  v1
I0209 23:31:09.572474    3152 ssh_runner.go:194] Run: docker version --format {{.Server.Version}}
I0209 23:31:12.560997    3152 ssh_runner.go:234] Completed: docker version --format {{.Server.Version}}: (2.99337332s)
I0209 23:31:12.561163    3152 ssh_runner.go:194] Run: docker version --format {{.Server.Version}}
I0209 23:31:12.814384    3152 out.go:252] üê≥  Preparing Kubernetes v1.35.0 on Docker 29.2.0 ...
I0209 23:31:12.814873    3152 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0209 23:31:12.897979    3152 ssh_runner.go:194] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0209 23:31:12.936953    3152 ssh_runner.go:194] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0209 23:31:12.955209    3152 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0209 23:31:13.009743    3152 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 Memory:2851 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.35.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.35.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s Rosetta:false} ...
I0209 23:31:13.069280    3152 preload.go:187] Checking if preload exists for k8s version v1.35.0 and runtime docker
I0209 23:31:13.069435    3152 ssh_runner.go:194] Run: docker images --format {{.Repository}}:{{.Tag}}
I0209 23:31:13.157819    3152 docker.go:693] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.35.0
registry.k8s.io/kube-proxy:v1.35.0
registry.k8s.io/kube-scheduler:v1.35.0
registry.k8s.io/kube-controller-manager:v1.35.0
registry.k8s.io/etcd:3.6.6-0
registry.k8s.io/coredns/coredns:v1.13.1
registry.k8s.io/pause:3.10.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0209 23:31:13.157844    3152 docker.go:623] Images already preloaded, skipping extraction
I0209 23:31:13.157990    3152 ssh_runner.go:194] Run: docker images --format {{.Repository}}:{{.Tag}}
I0209 23:31:13.206415    3152 docker.go:693] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.35.0
registry.k8s.io/kube-controller-manager:v1.35.0
registry.k8s.io/kube-proxy:v1.35.0
registry.k8s.io/kube-scheduler:v1.35.0
registry.k8s.io/etcd:3.6.6-0
registry.k8s.io/coredns/coredns:v1.13.1
registry.k8s.io/pause:3.10.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0209 23:31:13.206450    3152 cache_images.go:85] Images are preloaded, skipping loading
I0209 23:31:13.206471    3152 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.35.0 docker true true} ...
I0209 23:31:13.221716    3152 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.35.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.35.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0209 23:31:13.221982    3152 ssh_runner.go:194] Run: docker info --format {{.CgroupDriver}}
I0209 23:31:17.950970    3152 ssh_runner.go:234] Completed: docker info --format {{.CgroupDriver}}: (4.728945276s)
I0209 23:31:17.991122    3152 cni.go:83] Creating CNI manager for ""
I0209 23:31:17.991165    3152 cni.go:157] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0209 23:31:18.002335    3152 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0209 23:31:18.006664    3152 kubeadm.go:196] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.35.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0209 23:31:18.006849    3152 kubeadm.go:202] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
kubernetesVersion: v1.35.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0209 23:31:18.006982    3152 ssh_runner.go:194] Run: sudo ls /var/lib/minikube/binaries/v1.35.0
I0209 23:31:18.105438    3152 binaries.go:50] Found k8s binaries, skipping transfer
I0209 23:31:18.105630    3152 ssh_runner.go:194] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0209 23:31:18.126327    3152 ssh_runner.go:361] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0209 23:31:18.185723    3152 ssh_runner.go:361] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0209 23:31:18.214178    3152 ssh_runner.go:361] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2208 bytes)
I0209 23:31:18.238647    3152 ssh_runner.go:194] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0209 23:31:18.246022    3152 ssh_runner.go:194] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0209 23:31:18.265352    3152 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0209 23:31:18.528364    3152 ssh_runner.go:194] Run: sudo systemctl start kubelet
I0209 23:31:18.577267    3152 certs.go:67] Setting up /home/yasirtanoli/.minikube/profiles/minikube for IP: 192.168.49.2
I0209 23:31:18.577285    3152 certs.go:193] generating shared ca certs ...
I0209 23:31:18.577312    3152 certs.go:225] acquiring lock for ca certs: {Name:mk1058372310e58118ed59e62608d60c2ba14360 Timeout:1m0s Delay:500ms}
I0209 23:31:18.619006    3152 certs.go:234] skipping valid "minikubeCA" ca cert: /home/yasirtanoli/.minikube/ca.key
I0209 23:31:18.644021    3152 certs.go:234] skipping valid "proxyClientCA" ca cert: /home/yasirtanoli/.minikube/proxy-client-ca.key
I0209 23:31:18.644054    3152 certs.go:255] generating profile certs ...
I0209 23:31:18.644164    3152 certs.go:362] generating signed profile cert for "minikube-user": /home/yasirtanoli/.minikube/profiles/minikube/client.key
I0209 23:31:18.688036    3152 crypto.go:70] Generating cert /home/yasirtanoli/.minikube/profiles/minikube/client.crt with IP's: []
I0209 23:31:18.812025    3152 crypto.go:158] Writing cert to /home/yasirtanoli/.minikube/profiles/minikube/client.crt ...
I0209 23:31:18.812076    3152 lock.go:60] WriteFile acquiring /home/yasirtanoli/.minikube/profiles/minikube/client.crt: {Name:mk6f6c51ba3b3c7dd2e819d2d37a8e6b68112c35 Timeout:1m0s Delay:500ms}
I0209 23:31:18.857949    3152 crypto.go:166] Writing key to /home/yasirtanoli/.minikube/profiles/minikube/client.key ...
I0209 23:31:18.858032    3152 lock.go:60] WriteFile acquiring /home/yasirtanoli/.minikube/profiles/minikube/client.key: {Name:mke40ca399e4168df248a86902c3cb8a85a7d463 Timeout:1m0s Delay:500ms}
I0209 23:31:18.897790    3152 certs.go:362] generating signed profile cert for "minikube": /home/yasirtanoli/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0209 23:31:18.897888    3152 crypto.go:70] Generating cert /home/yasirtanoli/.minikube/profiles/minikube/apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0209 23:31:19.121155    3152 crypto.go:158] Writing cert to /home/yasirtanoli/.minikube/profiles/minikube/apiserver.crt.7fb57e3c ...
I0209 23:31:19.121185    3152 lock.go:60] WriteFile acquiring /home/yasirtanoli/.minikube/profiles/minikube/apiserver.crt.7fb57e3c: {Name:mk97fe7e16a7d4b769e4a75278577eca6825d216 Timeout:1m0s Delay:500ms}
I0209 23:31:19.121633    3152 crypto.go:166] Writing key to /home/yasirtanoli/.minikube/profiles/minikube/apiserver.key.7fb57e3c ...
I0209 23:31:19.121649    3152 lock.go:60] WriteFile acquiring /home/yasirtanoli/.minikube/profiles/minikube/apiserver.key.7fb57e3c: {Name:mkd33006d15b56cda853f84af8691d9ca537b6f7 Timeout:1m0s Delay:500ms}
I0209 23:31:19.121787    3152 certs.go:380] copying /home/yasirtanoli/.minikube/profiles/minikube/apiserver.crt.7fb57e3c -> /home/yasirtanoli/.minikube/profiles/minikube/apiserver.crt
I0209 23:31:19.185236    3152 certs.go:384] copying /home/yasirtanoli/.minikube/profiles/minikube/apiserver.key.7fb57e3c -> /home/yasirtanoli/.minikube/profiles/minikube/apiserver.key
I0209 23:31:19.185544    3152 certs.go:362] generating signed profile cert for "aggregator": /home/yasirtanoli/.minikube/profiles/minikube/proxy-client.key
I0209 23:31:19.185583    3152 crypto.go:70] Generating cert /home/yasirtanoli/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0209 23:31:19.299963    3152 crypto.go:158] Writing cert to /home/yasirtanoli/.minikube/profiles/minikube/proxy-client.crt ...
I0209 23:31:19.299987    3152 lock.go:60] WriteFile acquiring /home/yasirtanoli/.minikube/profiles/minikube/proxy-client.crt: {Name:mk2f8a6904af3d0b4d50850100997a10d8a54934 Timeout:1m0s Delay:500ms}
I0209 23:31:19.300236    3152 crypto.go:166] Writing key to /home/yasirtanoli/.minikube/profiles/minikube/proxy-client.key ...
I0209 23:31:19.300245    3152 lock.go:60] WriteFile acquiring /home/yasirtanoli/.minikube/profiles/minikube/proxy-client.key: {Name:mk6789a92a566ca15c79d25716e84870b54eef0b Timeout:1m0s Delay:500ms}
I0209 23:31:19.300602    3152 certs.go:482] found cert: /home/yasirtanoli/.minikube/certs/ca-key.pem (1675 bytes)
I0209 23:31:19.300661    3152 certs.go:482] found cert: /home/yasirtanoli/.minikube/certs/ca.pem (1090 bytes)
I0209 23:31:19.300700    3152 certs.go:482] found cert: /home/yasirtanoli/.minikube/certs/cert.pem (1135 bytes)
I0209 23:31:19.300739    3152 certs.go:482] found cert: /home/yasirtanoli/.minikube/certs/key.pem (1679 bytes)
I0209 23:31:19.542621    3152 ssh_runner.go:361] scp /home/yasirtanoli/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0209 23:31:19.603017    3152 ssh_runner.go:361] scp /home/yasirtanoli/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0209 23:31:19.637947    3152 ssh_runner.go:361] scp /home/yasirtanoli/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0209 23:31:19.681518    3152 ssh_runner.go:361] scp /home/yasirtanoli/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0209 23:31:19.715179    3152 ssh_runner.go:361] scp /home/yasirtanoli/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0209 23:31:19.748381    3152 ssh_runner.go:361] scp /home/yasirtanoli/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0209 23:31:19.783754    3152 ssh_runner.go:361] scp /home/yasirtanoli/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0209 23:31:19.817752    3152 ssh_runner.go:361] scp /home/yasirtanoli/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0209 23:31:19.853621    3152 ssh_runner.go:361] scp /home/yasirtanoli/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0209 23:31:19.907085    3152 ssh_runner.go:361] scp memory --> /var/lib/minikube/kubeconfig (722 bytes)
I0209 23:31:19.942464    3152 ssh_runner.go:194] Run: openssl version
I0209 23:31:20.043846    3152 ssh_runner.go:194] Run: sudo test -s /usr/share/ca-certificates/minikubeCA.pem
I0209 23:31:20.079901    3152 ssh_runner.go:194] Run: sudo ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem
I0209 23:31:20.146178    3152 ssh_runner.go:194] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0209 23:31:20.153415    3152 certs.go:526] hashing: -rw-r--r-- 1 root root 1111 Feb  9 11:47 /usr/share/ca-certificates/minikubeCA.pem
I0209 23:31:20.153534    3152 ssh_runner.go:194] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0209 23:31:20.432801    3152 ssh_runner.go:194] Run: sudo test -L /etc/ssl/certs/b5213941.0
I0209 23:31:20.460009    3152 ssh_runner.go:194] Run: sudo ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0
I0209 23:31:20.481890    3152 ssh_runner.go:194] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0209 23:31:20.488889    3152 certs.go:398] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0209 23:31:20.489081    3152 kubeadm.go:400] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 Memory:2851 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.35.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.35.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s Rosetta:false}
I0209 23:31:20.489297    3152 ssh_runner.go:194] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0209 23:31:20.526188    3152 ssh_runner.go:194] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0209 23:31:20.541150    3152 ssh_runner.go:194] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0209 23:31:20.555360    3152 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0209 23:31:20.555458    3152 ssh_runner.go:194] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0209 23:31:20.569753    3152 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0209 23:31:20.569790    3152 kubeadm.go:157] found existing configuration files:

I0209 23:31:20.569886    3152 ssh_runner.go:194] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0209 23:31:20.585044    3152 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0209 23:31:20.585187    3152 ssh_runner.go:194] Run: sudo rm -f /etc/kubernetes/admin.conf
I0209 23:31:20.599259    3152 ssh_runner.go:194] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0209 23:31:20.613873    3152 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0209 23:31:20.614032    3152 ssh_runner.go:194] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0209 23:31:20.628004    3152 ssh_runner.go:194] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0209 23:31:20.642576    3152 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0209 23:31:20.642689    3152 ssh_runner.go:194] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0209 23:31:20.656702    3152 ssh_runner.go:194] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0209 23:31:20.671359    3152 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0209 23:31:20.671472    3152 ssh_runner.go:194] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0209 23:31:20.690677    3152 ssh_runner.go:285] Start: sudo /bin/bash -c "env PATH="/var/lib/minikube/binaries/v1.35.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0209 23:31:28.044424    3152 kubeadm.go:318] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0209 23:31:28.608484    3152 kubeadm.go:318] 	[WARNING Service-kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0209 23:33:39.885292    3152 kubeadm.go:318] [init] Using Kubernetes version: v1.35.0
I0209 23:33:39.885399    3152 kubeadm.go:318] [preflight] Running pre-flight checks
I0209 23:33:39.885557    3152 kubeadm.go:318] [preflight] Pulling images required for setting up a Kubernetes cluster
I0209 23:33:39.885696    3152 kubeadm.go:318] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0209 23:33:39.885792    3152 kubeadm.go:318] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0209 23:33:39.885886    3152 kubeadm.go:318] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0209 23:33:40.270113    3152 out.go:252]     ‚ñ™ Generating certificates and keys ...
I0209 23:33:40.270439    3152 kubeadm.go:318] [certs] Using existing ca certificate authority
I0209 23:33:40.270567    3152 kubeadm.go:318] [certs] Using existing apiserver certificate and key on disk
I0209 23:33:40.270694    3152 kubeadm.go:318] [certs] Generating "apiserver-kubelet-client" certificate and key
I0209 23:33:40.270818    3152 kubeadm.go:318] [certs] Generating "front-proxy-ca" certificate and key
I0209 23:33:40.270936    3152 kubeadm.go:318] [certs] Generating "front-proxy-client" certificate and key
I0209 23:33:40.271043    3152 kubeadm.go:318] [certs] Generating "etcd/ca" certificate and key
I0209 23:33:40.271168    3152 kubeadm.go:318] [certs] Generating "etcd/server" certificate and key
I0209 23:33:40.271408    3152 kubeadm.go:318] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0209 23:33:40.271514    3152 kubeadm.go:318] [certs] Generating "etcd/peer" certificate and key
I0209 23:33:40.271819    3152 kubeadm.go:318] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0209 23:33:40.271959    3152 kubeadm.go:318] [certs] Generating "etcd/healthcheck-client" certificate and key
I0209 23:33:40.272033    3152 kubeadm.go:318] [certs] Generating "apiserver-etcd-client" certificate and key
I0209 23:33:40.272106    3152 kubeadm.go:318] [certs] Generating "sa" key and public key
I0209 23:33:40.272173    3152 kubeadm.go:318] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0209 23:33:40.272239    3152 kubeadm.go:318] [kubeconfig] Writing "admin.conf" kubeconfig file
I0209 23:33:40.272303    3152 kubeadm.go:318] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0209 23:33:40.272352    3152 kubeadm.go:318] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0209 23:33:40.272433    3152 kubeadm.go:318] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0209 23:33:40.272501    3152 kubeadm.go:318] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0209 23:33:40.272601    3152 kubeadm.go:318] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0209 23:33:40.272695    3152 kubeadm.go:318] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0209 23:33:40.627057    3152 out.go:252]     ‚ñ™ Booting up control plane ...
I0209 23:33:40.627546    3152 kubeadm.go:318] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0209 23:33:40.627741    3152 kubeadm.go:318] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0209 23:33:40.627928    3152 kubeadm.go:318] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0209 23:33:40.628303    3152 kubeadm.go:318] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0209 23:33:40.628612    3152 kubeadm.go:318] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/instance-config.yaml"
I0209 23:33:40.628922    3152 kubeadm.go:318] [patches] Applied patch of type "application/strategic-merge-patch+json" to target "kubeletconfiguration"
I0209 23:33:40.629107    3152 kubeadm.go:318] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0209 23:33:40.629262    3152 kubeadm.go:318] [kubelet-start] Starting the kubelet
I0209 23:33:40.629646    3152 kubeadm.go:318] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0209 23:33:40.629941    3152 kubeadm.go:318] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0209 23:33:40.630062    3152 kubeadm.go:318] [kubelet-check] The kubelet is healthy after 1.072558996s
I0209 23:33:40.630267    3152 kubeadm.go:318] [control-plane-check] Waiting for healthy control plane components. This can take up to 4m0s
I0209 23:33:40.630479    3152 kubeadm.go:318] [control-plane-check] Checking kube-apiserver at https://192.168.49.2:8443/livez
I0209 23:33:40.630713    3152 kubeadm.go:318] [control-plane-check] Checking kube-controller-manager at https://127.0.0.1:10257/healthz
I0209 23:33:40.630865    3152 kubeadm.go:318] [control-plane-check] Checking kube-scheduler at https://127.0.0.1:10259/livez
I0209 23:33:40.631046    3152 kubeadm.go:318] [control-plane-check] kube-scheduler is healthy after 36.12326845s
I0209 23:33:40.631226    3152 kubeadm.go:318] [control-plane-check] kube-controller-manager is healthy after 43.623550138s
I0209 23:33:40.631457    3152 kubeadm.go:318] [control-plane-check] kube-apiserver is healthy after 1m58.502279182s
I0209 23:33:40.631623    3152 kubeadm.go:318] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0209 23:33:40.631956    3152 kubeadm.go:318] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0209 23:33:40.632109    3152 kubeadm.go:318] [upload-certs] Skipping phase. Please see --upload-certs
I0209 23:33:40.632404    3152 kubeadm.go:318] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0209 23:33:40.632502    3152 kubeadm.go:318] [bootstrap-token] Using token: 0nqrdt.uwdi19nuc04p2wn3
I0209 23:33:40.804030    3152 out.go:252]     ‚ñ™ Configuring RBAC rules ...
I0209 23:33:40.804485    3152 kubeadm.go:318] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0209 23:33:40.804715    3152 kubeadm.go:318] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0209 23:33:40.805148    3152 kubeadm.go:318] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0209 23:33:40.805528    3152 kubeadm.go:318] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0209 23:33:40.805907    3152 kubeadm.go:318] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0209 23:33:40.806082    3152 kubeadm.go:318] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0209 23:33:40.806360    3152 kubeadm.go:318] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0209 23:33:40.806537    3152 kubeadm.go:318] [addons] Applied essential addon: CoreDNS
I0209 23:33:40.806685    3152 kubeadm.go:318] [addons] Applied essential addon: kube-proxy
I0209 23:33:40.806782    3152 kubeadm.go:318] 
I0209 23:33:40.806937    3152 kubeadm.go:318] Your Kubernetes control-plane has initialized successfully!
I0209 23:33:40.806946    3152 kubeadm.go:318] 
I0209 23:33:40.807131    3152 kubeadm.go:318] To start using your cluster, you need to run the following as a regular user:
I0209 23:33:40.807136    3152 kubeadm.go:318] 
I0209 23:33:40.807165    3152 kubeadm.go:318]   mkdir -p $HOME/.kube
I0209 23:33:40.807258    3152 kubeadm.go:318]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0209 23:33:40.807321    3152 kubeadm.go:318]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0209 23:33:40.807325    3152 kubeadm.go:318] 
I0209 23:33:40.807397    3152 kubeadm.go:318] Alternatively, if you are the root user, you can run:
I0209 23:33:40.807404    3152 kubeadm.go:318] 
I0209 23:33:40.807482    3152 kubeadm.go:318]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0209 23:33:40.807486    3152 kubeadm.go:318] 
I0209 23:33:40.807613    3152 kubeadm.go:318] You should now deploy a pod network to the cluster.
I0209 23:33:40.807718    3152 kubeadm.go:318] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0209 23:33:40.807786    3152 kubeadm.go:318]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0209 23:33:40.807790    3152 kubeadm.go:318] 
I0209 23:33:40.807913    3152 kubeadm.go:318] You can now join any number of control-plane nodes by copying certificate authorities
I0209 23:33:40.808009    3152 kubeadm.go:318] and service account keys on each node and then running the following as root:
I0209 23:33:40.808013    3152 kubeadm.go:318] 
I0209 23:33:40.808173    3152 kubeadm.go:318]   kubeadm join control-plane.minikube.internal:8443 --token 0nqrdt.uwdi19nuc04p2wn3 \
I0209 23:33:40.808308    3152 kubeadm.go:318] 	--discovery-token-ca-cert-hash sha256:e1a0ceaefabb995b00b3a250fc027dda32980927bd56c684bfe412b38d95afc0 \
I0209 23:33:40.808331    3152 kubeadm.go:318] 	--control-plane 
I0209 23:33:40.808335    3152 kubeadm.go:318] 
I0209 23:33:40.808448    3152 kubeadm.go:318] Then you can join any number of worker nodes by running the following on each as root:
I0209 23:33:40.808454    3152 kubeadm.go:318] 
I0209 23:33:40.808570    3152 kubeadm.go:318] kubeadm join control-plane.minikube.internal:8443 --token 0nqrdt.uwdi19nuc04p2wn3 \
I0209 23:33:40.808891    3152 kubeadm.go:318] 	--discovery-token-ca-cert-hash sha256:e1a0ceaefabb995b00b3a250fc027dda32980927bd56c684bfe412b38d95afc0 
I0209 23:33:40.808991    3152 cni.go:83] Creating CNI manager for ""
I0209 23:33:40.809028    3152 cni.go:157] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0209 23:33:41.085787    3152 out.go:179] üîó  Configuring bridge CNI (Container Networking Interface) ...
I0209 23:33:41.235966    3152 ssh_runner.go:194] Run: sudo mkdir -p /etc/cni/net.d
I0209 23:33:42.028016    3152 ssh_runner.go:361] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0209 23:33:42.103488    3152 ssh_runner.go:194] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0209 23:33:42.163760    3152 ssh_runner.go:194] Run: sudo /var/lib/minikube/binaries/v1.35.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0209 23:33:42.214614    3152 ssh_runner.go:194] Run: sudo /var/lib/minikube/binaries/v1.35.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2026_02_09T23_33_42_0700 minikube.k8s.io/version=v1.38.0 minikube.k8s.io/commit=de81223c61ab1bd97dcfcfa6d9d5c59e5da4a0cf minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0209 23:33:42.404175    3152 ops.go:34] apiserver oom_adj: -16
I0209 23:34:09.140518    3152 ssh_runner.go:234] Completed: sudo /var/lib/minikube/binaries/v1.35.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig: (26.976568167s)
I0209 23:34:09.140635    3152 ssh_runner.go:234] Completed: sudo /var/lib/minikube/binaries/v1.35.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2026_02_09T23_33_42_0700 minikube.k8s.io/version=v1.38.0 minikube.k8s.io/commit=de81223c61ab1bd97dcfcfa6d9d5c59e5da4a0cf minikube.k8s.io/name=minikube minikube.k8s.io/primary=true: (26.92587676s)
I0209 23:34:09.140691    3152 kubeadm.go:1113] duration metric: took 26.977206457s to wait for elevateKubeSystemPrivileges
I0209 23:34:09.140799    3152 kubeadm.go:402] duration metric: took 2m48.671582144s to StartCluster
I0209 23:34:09.140869    3152 settings.go:141] acquiring lock: {Name:mk9f6d212ba60cb02307050f8813cfe0b2b88dca Timeout:1m0s Delay:500ms}
I0209 23:34:09.141282    3152 settings.go:149] Updating kubeconfig:  /home/yasirtanoli/.kube/config
I0209 23:34:09.933872    3152 lock.go:60] WriteFile acquiring /home/yasirtanoli/.kube/config: {Name:mkb8a478ed0c69aa6af0eb3879d86ef93aae40c2 Timeout:1m0s Delay:500ms}
I0209 23:34:10.097450    3152 config.go:183] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.35.0
I0209 23:34:10.606357    3152 addons.go:528] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubetail:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0209 23:34:10.749804    3152 addons.go:71] Setting storage-provisioner=true in profile "minikube"
I0209 23:34:10.749823    3152 addons.go:71] Setting default-storageclass=true in profile "minikube"
I0209 23:34:10.749853    3152 addons.go:240] Setting addon storage-provisioner=true in "minikube"
I0209 23:34:10.749859    3152 addons_storage_classes.go:34] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0209 23:34:10.766074    3152 start.go:237] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.35.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0209 23:34:10.974535    3152 host.go:67] Checking if "minikube" exists ...
I0209 23:34:11.146956    3152 out.go:179] üîé  Verifying Kubernetes components...
I0209 23:34:11.360833    3152 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0209 23:34:11.361248    3152 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0209 23:34:11.491971    3152 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0209 23:34:13.371643    3152 ssh_runner.go:234] Completed: sudo systemctl daemon-reload: (1.879636431s)
I0209 23:34:13.371790    3152 ssh_runner.go:194] Run: sudo systemctl start kubelet
I0209 23:34:13.397553    3152 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
W0209 23:34:27.129577    3152 out.go:285] ‚ùó  Executing "docker container inspect minikube --format={{.State.Status}}" took an unusually long time: 15.768074709s
W0209 23:34:27.129649    3152 out.go:285] üí°  Restarting the docker service may improve performance.
I0209 23:34:27.129997    3152 cli_runner.go:217] Completed: docker container inspect minikube --format={{.State.Status}}: (15.768074709s)
I0209 23:34:27.133041    3152 cli_runner.go:217] Completed: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube: (13.735412182s)
I0209 23:34:27.301581    3152 cli_runner.go:217] Completed: docker container inspect minikube --format={{.State.Status}}: (15.940681117s)
I0209 23:34:27.907653    3152 out.go:179]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0209 23:34:28.218976    3152 addons.go:437] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0209 23:34:28.218991    3152 ssh_runner.go:361] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0209 23:34:28.219087    3152 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0209 23:34:28.277097    3152 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63522 SSHKeyPath:/home/yasirtanoli/.minikube/machines/minikube/id_rsa Username:docker}
I0209 23:34:40.783575    3152 addons.go:240] Setting addon default-storageclass=true in "minikube"
I0209 23:34:40.783771    3152 host.go:67] Checking if "minikube" exists ...
I0209 23:34:40.784521    3152 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0209 23:34:40.867481    3152 addons.go:437] installing /etc/kubernetes/addons/storageclass.yaml
I0209 23:34:40.867513    3152 ssh_runner.go:361] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0209 23:34:40.867677    3152 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0209 23:34:41.427819    3152 api_server.go:51] waiting for apiserver process to appear ...
I0209 23:34:41.427943    3152 ssh_runner.go:194] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0209 23:34:41.475445    3152 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63522 SSHKeyPath:/home/yasirtanoli/.minikube/machines/minikube/id_rsa Username:docker}
I0209 23:34:41.477628    3152 api_server.go:71] duration metric: took 30.720360545s to wait for apiserver process to appear ...
I0209 23:34:41.477667    3152 api_server.go:87] waiting for apiserver healthz status ...
I0209 23:34:41.477695    3152 api_server.go:298] Checking apiserver healthz at https://127.0.0.1:63521/healthz ...
I0209 23:34:46.745742    3152 api_server.go:324] https://127.0.0.1:63521/healthz returned 200:
ok
I0209 23:34:47.883849    3152 api_server.go:140] control plane version: v1.35.0
I0209 23:34:47.883874    3152 api_server.go:130] duration metric: took 6.406197976s to wait for apiserver health ...
I0209 23:34:47.883888    3152 system_pods.go:42] waiting for kube-system pods to appear ...
I0209 23:34:52.129607    3152 ssh_runner.go:194] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.35.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0209 23:34:52.436055    3152 ssh_runner.go:194] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.35.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0209 23:34:53.430359    3152 system_pods.go:58] 7 kube-system pods found
I0209 23:34:53.430664    3152 system_pods.go:60] "coredns-7d764666f9-4nzzl" [c0c80d30-8993-43cd-beca-ee4695956110] Pending / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0209 23:34:53.430722    3152 system_pods.go:60] "coredns-7d764666f9-99hn5" [2ba21067-733e-4ecf-bdba-7d4e35df653e] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0209 23:34:53.430735    3152 system_pods.go:60] "etcd-minikube" [415205fe-27ba-4d0a-a9fc-748b9d8a21d7] Running
I0209 23:34:53.430742    3152 system_pods.go:60] "kube-apiserver-minikube" [fff01d9e-d529-4a81-901d-62004b327804] Running
I0209 23:34:53.430748    3152 system_pods.go:60] "kube-controller-manager-minikube" [7086be3d-05e7-4f9a-a8e4-3b0b8062e9e4] Running
I0209 23:34:53.430754    3152 system_pods.go:60] "kube-proxy-m4zrn" [c1854297-dd3d-43d3-adca-fbe661b3fc0e] Running
I0209 23:34:53.430759    3152 system_pods.go:60] "kube-scheduler-minikube" [68365e52-9737-45bf-9edc-fb0ee2536b65] Running
I0209 23:34:53.430767    3152 system_pods.go:73] duration metric: took 5.546870337s to wait for pod list to return data ...
I0209 23:34:53.430792    3152 kubeadm.go:586] duration metric: took 42.673526471s to wait for: map[apiserver:true system_pods:true]
I0209 23:34:53.430819    3152 node_conditions.go:101] verifying NodePressure condition ...
I0209 23:34:56.314265    3152 node_conditions.go:121] node storage ephemeral capacity is 1055762868Ki
I0209 23:34:56.314300    3152 node_conditions.go:122] node cpu capacity is 4
I0209 23:34:56.352731    3152 node_conditions.go:104] duration metric: took 2.92189161s to run NodePressure ...
I0209 23:34:56.352763    3152 start.go:243] waiting for startup goroutines ...
I0209 23:35:14.623398    3152 ssh_runner.go:234] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.35.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (22.507062629s)
I0209 23:35:14.623624    3152 ssh_runner.go:234] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.35.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (22.200808091s)
I0209 23:35:19.171833    3152 out.go:179] üåü  Enabled addons: storage-provisioner, default-storageclass
I0209 23:35:20.033744    3152 addons.go:531] duration metric: took 1m9.449605656s for enable addons: enabled=[storage-provisioner default-storageclass]
I0209 23:35:20.033830    3152 start.go:248] waiting for cluster config update ...
I0209 23:35:20.033849    3152 start.go:257] writing updated cluster config ...
I0209 23:35:20.145687    3152 ssh_runner.go:194] Run: rm -f paused
I0209 23:36:37.193269    3152 start.go:629] kubectl: 1.34.1, cluster: 1.35.0 (minor skew: 1)
I0209 23:36:37.591067    3152 out.go:179] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Feb 09 18:37:36 minikube cri-dockerd[2180]: time="2026-02-09T18:37:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/567d7338661bde91a72350cc16b50317158c66e8af4dd0a064513886f67a6379/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 09 18:37:36 minikube cri-dockerd[2180]: time="2026-02-09T18:37:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/715a10e973ec66276708d8c3ab0edc974ce50bd3e560ceb973b5222e2fd55763/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 09 18:37:44 minikube dockerd[972]: time="2026-02-09T18:37:44.387029568Z" level=warning msg="reference for unknown type: " digest="sha256:03a00eb0e255e8a25fa49926c24cde0f7e12e8d072c445cdf5136ec78b546285" remote="registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:03a00eb0e255e8a25fa49926c24cde0f7e12e8d072c445cdf5136ec78b546285"
Feb 09 18:37:59 minikube cri-dockerd[2180]: time="2026-02-09T18:37:59Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.5@sha256:03a00eb0e255e8a25fa49926c24cde0f7e12e8d072c445cdf5136ec78b546285: 0ea18369964b: Downloading [=======>                                           ]  2.679MB/18.04MB"
Feb 09 18:38:09 minikube cri-dockerd[2180]: time="2026-02-09T18:38:09Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.5@sha256:03a00eb0e255e8a25fa49926c24cde0f7e12e8d072c445cdf5136ec78b546285: 0ea18369964b: Downloading [=================================>                 ]  12.25MB/18.04MB"
Feb 09 18:38:20 minikube cri-dockerd[2180]: time="2026-02-09T18:38:19Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.5@sha256:03a00eb0e255e8a25fa49926c24cde0f7e12e8d072c445cdf5136ec78b546285: 0ea18369964b: Extracting [=========================================>         ]  14.94MB/18.04MB"
Feb 09 18:38:24 minikube cri-dockerd[2180]: time="2026-02-09T18:38:24Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.5@sha256:03a00eb0e255e8a25fa49926c24cde0f7e12e8d072c445cdf5136ec78b546285: Status: Downloaded newer image for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:03a00eb0e255e8a25fa49926c24cde0f7e12e8d072c445cdf5136ec78b546285"
Feb 09 18:38:33 minikube cri-dockerd[2180]: time="2026-02-09T18:38:33Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.5@sha256:03a00eb0e255e8a25fa49926c24cde0f7e12e8d072c445cdf5136ec78b546285: Status: Image is up to date for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:03a00eb0e255e8a25fa49926c24cde0f7e12e8d072c445cdf5136ec78b546285"
Feb 09 18:38:40 minikube dockerd[972]: time="2026-02-09T18:38:40.637457220Z" level=info msg="ignoring event" container=3db753d2251386d5adfd8b803464734243df995b8b3b613319bb8b14429251f8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 09 18:38:41 minikube dockerd[972]: time="2026-02-09T18:38:41.680837266Z" level=info msg="ignoring event" container=4be02c54013a09bf26f65330deeb713a1229ae87ad4d3c4e0157f29a475dfb38 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 09 18:38:49 minikube dockerd[972]: time="2026-02-09T18:38:49.478464118Z" level=info msg="ignoring event" container=715a10e973ec66276708d8c3ab0edc974ce50bd3e560ceb973b5222e2fd55763 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 09 18:39:48 minikube dockerd[972]: time="2026-02-09T18:39:48.573802967Z" level=info msg="sbJoin: gwep4 ''->'', gwep6 ''->''" eid=351b35e16aa4 ep=k8s_POD_ingress-nginx-controller-8675c6b56f-8xp54_ingress-nginx_86f2a588-e231-44e5-b073-fab9793ce535_0 net=none nid=55ff82ceb211
Feb 09 18:39:49 minikube cri-dockerd[2180]: time="2026-02-09T18:39:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/59576c067c89deba9a6724f23f0589178294d2a0cf8327744253ed3d3926bb3e/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 09 18:39:52 minikube dockerd[972]: time="2026-02-09T18:39:52.535807538Z" level=warning msg="reference for unknown type: " digest="sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47" remote="registry.k8s.io/ingress-nginx/controller@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47"
Feb 09 18:40:13 minikube cri-dockerd[2180]: time="2026-02-09T18:40:13Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 2d35ebdb57d9: Downloading [==>                                                ]  200.7kB/3.802MB"
Feb 09 18:40:22 minikube cri-dockerd[2180]: time="2026-02-09T18:40:22Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: b0abb25b0f0f: Downloading [============>                                      ]  1.263MB/5.153MB"
Feb 09 18:40:32 minikube cri-dockerd[2180]: time="2026-02-09T18:40:32Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 2d35ebdb57d9: Downloading [============================>                      ]  2.133MB/3.802MB"
Feb 09 18:40:42 minikube cri-dockerd[2180]: time="2026-02-09T18:40:42Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 65d72828f76d: Downloading [========>                                          ]  5.541MB/33.68MB"
Feb 09 18:40:50 minikube cri-dockerd[2180]: time="2026-02-09T18:40:50Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 65d72828f76d: Downloading [==========>                                        ]  6.917MB/33.68MB"
Feb 09 18:41:01 minikube cri-dockerd[2180]: time="2026-02-09T18:41:01Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 65d72828f76d: Downloading [==================>                                ]  12.42MB/33.68MB"
Feb 09 18:41:11 minikube cri-dockerd[2180]: time="2026-02-09T18:41:11Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 65d72828f76d: Downloading [=========================>                         ]   16.9MB/33.68MB"
Feb 09 18:41:22 minikube cri-dockerd[2180]: time="2026-02-09T18:41:22Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 154d1a759bca: Downloading [>                                                  ]  50.68kB/4.82MB"
Feb 09 18:41:26 minikube cri-dockerd[2180]: time="2026-02-09T18:41:26Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 65d72828f76d: Downloading [============================>                      ]   19.3MB/33.68MB"
Feb 09 18:41:41 minikube cri-dockerd[2180]: time="2026-02-09T18:41:41Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 154d1a759bca: Downloading [=>                                                 ]  102.9kB/4.82MB"
Feb 09 18:41:45 minikube cri-dockerd[2180]: time="2026-02-09T18:41:45Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 154d1a759bca: Downloading [=>                                                 ]  155.1kB/4.82MB"
Feb 09 18:41:56 minikube dockerd[972]: time="2026-02-09T18:41:56.612384288Z" level=info msg="Download failed, retrying (1/5): net/http: TLS handshake timeout"
Feb 09 18:42:01 minikube cri-dockerd[2180]: time="2026-02-09T18:42:01Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 7d95f7f15f87: Retrying in 1 second "
Feb 09 18:42:07 minikube cri-dockerd[2180]: time="2026-02-09T18:42:07Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 65d72828f76d: Downloading [=================================>                 ]  22.74MB/33.68MB"
Feb 09 18:42:20 minikube cri-dockerd[2180]: time="2026-02-09T18:42:20Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 65d72828f76d: Downloading [=================================>                 ]  22.74MB/33.68MB"
Feb 09 18:42:30 minikube cri-dockerd[2180]: time="2026-02-09T18:42:30Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 65d72828f76d: Downloading [=================================>                 ]  22.74MB/33.68MB"
Feb 09 18:42:40 minikube dockerd[972]: time="2026-02-09T18:42:40.282449272Z" level=info msg="Download failed, retrying (2/5): dial tcp 34.96.108.209:443: connect: connection refused"
Feb 09 18:42:40 minikube cri-dockerd[2180]: time="2026-02-09T18:42:40Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 7d95f7f15f87: Retrying in 10 seconds "
Feb 09 18:42:50 minikube cri-dockerd[2180]: time="2026-02-09T18:42:50Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 7d95f7f15f87: Retrying in 1 second "
Feb 09 18:43:00 minikube cri-dockerd[2180]: time="2026-02-09T18:43:00Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 7d95f7f15f87: Retrying in 1 second "
Feb 09 18:43:10 minikube cri-dockerd[2180]: time="2026-02-09T18:43:10Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 7d95f7f15f87: Retrying in 1 second "
Feb 09 18:43:16 minikube cri-dockerd[2180]: time="2026-02-09T18:43:16Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 7d95f7f15f87: Retrying in 1 second "
Feb 09 18:43:26 minikube cri-dockerd[2180]: time="2026-02-09T18:43:26Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 65d72828f76d: Downloading [=======================================>           ]  26.53MB/33.68MB"
Feb 09 18:43:36 minikube cri-dockerd[2180]: time="2026-02-09T18:43:36Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 65d72828f76d: Downloading [=======================================>           ]  26.87MB/33.68MB"
Feb 09 18:43:49 minikube cri-dockerd[2180]: time="2026-02-09T18:43:49Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 65d72828f76d: Downloading [=========================================>         ]  28.25MB/33.68MB"
Feb 09 18:44:00 minikube cri-dockerd[2180]: time="2026-02-09T18:44:00Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 65d72828f76d: Downloading [==============================================>    ]     31MB/33.68MB"
Feb 09 18:44:10 minikube cri-dockerd[2180]: time="2026-02-09T18:44:10Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 65d72828f76d: Extracting [=========>                                         ]  6.488MB/33.68MB"
Feb 09 18:44:19 minikube cri-dockerd[2180]: time="2026-02-09T18:44:19Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 65d72828f76d: Pull complete "
Feb 09 18:44:29 minikube cri-dockerd[2180]: time="2026-02-09T18:44:29Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 7d95f7f15f87: Downloading  19.99MB"
Feb 09 18:44:39 minikube cri-dockerd[2180]: time="2026-02-09T18:44:39Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: fbe2de15214b: Downloading [======>                                            ]  348.3kB/2.868MB"
Feb 09 18:44:50 minikube cri-dockerd[2180]: time="2026-02-09T18:44:50Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: fbe2de15214b: Downloading [===============================================>   ]  2.751MB/2.868MB"
Feb 09 18:45:00 minikube cri-dockerd[2180]: time="2026-02-09T18:45:00Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 419e18693337: Downloading [======================>                            ]  7.053MB/15.95MB"
Feb 09 18:45:10 minikube cri-dockerd[2180]: time="2026-02-09T18:45:10Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 419e18693337: Downloading [================================>                  ]  10.49MB/15.95MB"
Feb 09 18:45:20 minikube cri-dockerd[2180]: time="2026-02-09T18:45:20Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 419e18693337: Downloading [=================================>                 ]  10.66MB/15.95MB"
Feb 09 18:45:31 minikube cri-dockerd[2180]: time="2026-02-09T18:45:31Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 66d55aa408c4: Download complete "
Feb 09 18:45:41 minikube cri-dockerd[2180]: time="2026-02-09T18:45:41Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: c765003409c8: Downloading  540.4kB"
Feb 09 18:45:51 minikube cri-dockerd[2180]: time="2026-02-09T18:45:51Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 419e18693337: Extracting [=======================>                           ]  7.537MB/15.95MB"
Feb 09 18:46:01 minikube cri-dockerd[2180]: time="2026-02-09T18:46:01Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 419e18693337: Extracting [==================================================>]  15.95MB/15.95MB"
Feb 09 18:46:11 minikube cri-dockerd[2180]: time="2026-02-09T18:46:11Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: c765003409c8: Download complete "
Feb 09 18:46:21 minikube cri-dockerd[2180]: time="2026-02-09T18:46:21Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: c765003409c8: Download complete "
Feb 09 18:46:33 minikube cri-dockerd[2180]: time="2026-02-09T18:46:31Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 2b601845d796: Pull complete "
Feb 09 18:46:40 minikube dockerd[972]: time="2026-02-09T18:46:39.534282718Z" level=info msg="ignoring event" container=bc02f5535f6b6a77ec654204a1f59d66a16b4dbc7bd50da5e093c8d702a169f6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 09 18:46:41 minikube cri-dockerd[2180]: time="2026-02-09T18:46:41Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: c765003409c8: Extracting  6.685MB"
Feb 09 18:46:51 minikube cri-dockerd[2180]: time="2026-02-09T18:46:51Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: c765003409c8: Extracting  22.29MB"
Feb 09 18:47:01 minikube cri-dockerd[2180]: time="2026-02-09T18:47:01Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: b7ec11173ab8: Extracting [==================================================>]     211B/211B"
Feb 09 18:47:09 minikube cri-dockerd[2180]: time="2026-02-09T18:47:09Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: Status: Downloaded newer image for registry.k8s.io/ingress-nginx/controller@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47"


==> container status <==
CONTAINER           IMAGE                                                                                                                        CREATED              STATE               NAME                      ATTEMPT             POD ID              POD                                         NAMESPACE
af8e2c79cf55a       registry.k8s.io/ingress-nginx/controller@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47             About a minute ago   Running             controller                0                   59576c067c89d       ingress-nginx-controller-8675c6b56f-8xp54   ingress-nginx
8be3c06ec3908       6e38f40d628db                                                                                                                About a minute ago   Running             storage-provisioner       1                   21599b30c75aa       storage-provisioner                         kube-system
4be02c54013a0       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:03a00eb0e255e8a25fa49926c24cde0f7e12e8d072c445cdf5136ec78b546285   10 minutes ago       Exited              patch                     0                   715a10e973ec6       ingress-nginx-admission-patch-wmtfv         ingress-nginx
3db753d225138       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:03a00eb0e255e8a25fa49926c24cde0f7e12e8d072c445cdf5136ec78b546285   10 minutes ago       Exited              create                    0                   567d7338661bd       ingress-nginx-admission-create-x2knr        ingress-nginx
5d253069c629e       aa5e3ebc0dfed                                                                                                                12 minutes ago       Running             coredns                   1                   86ef995f087db       coredns-7d764666f9-4nzzl                    kube-system
b072fee408d27       aa5e3ebc0dfed                                                                                                                12 minutes ago       Running             coredns                   1                   b0507e4eaaa7c       coredns-7d764666f9-99hn5                    kube-system
bc02f5535f6b6       6e38f40d628db                                                                                                                12 minutes ago       Exited              storage-provisioner       0                   21599b30c75aa       storage-provisioner                         kube-system
c0bd8584a5615       aa5e3ebc0dfed                                                                                                                14 minutes ago       Exited              coredns                   0                   86ef995f087db       coredns-7d764666f9-4nzzl                    kube-system
2b488cb02397e       aa5e3ebc0dfed                                                                                                                14 minutes ago       Exited              coredns                   0                   b0507e4eaaa7c       coredns-7d764666f9-99hn5                    kube-system
47ba7b1cb1175       32652ff1bbe6b                                                                                                                14 minutes ago       Running             kube-proxy                0                   61fd890c093c4       kube-proxy-m4zrn                            kube-system
35cbf93a24407       2c9a4b058bd7e                                                                                                                15 minutes ago       Running             kube-controller-manager   3                   1da5825110ac7       kube-controller-manager-minikube            kube-system
b6b331fe63275       2c9a4b058bd7e                                                                                                                15 minutes ago       Exited              kube-controller-manager   2                   1da5825110ac7       kube-controller-manager-minikube            kube-system
c7fa8edd7d6ed       5c6acd67e9cd1                                                                                                                15 minutes ago       Running             kube-apiserver            1                   187db9899b82a       kube-apiserver-minikube                     kube-system
3ad6fc9caff93       0a108f7189562                                                                                                                16 minutes ago       Running             etcd                      0                   8e28f2c524676       etcd-minikube                               kube-system
cb5a0c27b5ff1       5c6acd67e9cd1                                                                                                                16 minutes ago       Exited              kube-apiserver            0                   187db9899b82a       kube-apiserver-minikube                     kube-system
ed7d6b75b3c51       550794e3b12ac                                                                                                                16 minutes ago       Running             kube-scheduler            0                   096557e463450       kube-scheduler-minikube                     kube-system


==> coredns [2b488cb02397] <==
maxprocs: Leaving GOMAXPROCS=4: CPU quota undefined
[INFO] SIGTERM: Shutting down servers then terminating


==> coredns [5d253069c629] <==
maxprocs: Leaving GOMAXPROCS=4: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.13.1
linux/amd64, go1.25.2, 1db4568
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.250995607s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.60528914s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)


==> coredns [b072fee408d2] <==
maxprocs: Leaving GOMAXPROCS=4: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.13.1
linux/amd64, go1.25.2, 1db4568
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.250918293s


==> coredns [c0bd8584a561] <==
maxprocs: Leaving GOMAXPROCS=4: CPU quota undefined
[INFO] SIGTERM: Shutting down servers then terminating


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=de81223c61ab1bd97dcfcfa6d9d5c59e5da4a0cf
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2026_02_09T23_33_42_0700
                    minikube.k8s.io/version=v1.38.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 09 Feb 2026 18:32:24 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 09 Feb 2026 18:49:15 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 09 Feb 2026 18:47:30 +0000   Mon, 09 Feb 2026 18:32:20 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 09 Feb 2026 18:47:30 +0000   Mon, 09 Feb 2026 18:32:20 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 09 Feb 2026 18:47:30 +0000   Mon, 09 Feb 2026 18:32:20 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 09 Feb 2026 18:47:30 +0000   Mon, 09 Feb 2026 18:33:54 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-2Mi:      0
  memory:             2919556Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-2Mi:      0
  memory:             2919556Ki
  pods:               110
System Info:
  Machine ID:                 4571ab41f5a9a03217021e966978f901
  System UUID:                4571ab41f5a9a03217021e966978f901
  Boot ID:                    99e92d18-efa3-4f7e-97e3-2c08d8f52ff7
  Kernel Version:             6.6.114.1-microsoft-standard-WSL2
  OS Image:                   Debian GNU/Linux 12 (bookworm)
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://29.2.0
  Kubelet Version:            v1.35.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  ingress-nginx               ingress-nginx-controller-8675c6b56f-8xp54    100m (2%)     0 (0%)      90Mi (3%)        0 (0%)         11m
  kube-system                 coredns-7d764666f9-4nzzl                     100m (2%)     0 (0%)      70Mi (2%)        170Mi (5%)     15m
  kube-system                 coredns-7d764666f9-99hn5                     100m (2%)     0 (0%)      70Mi (2%)        170Mi (5%)     15m
  kube-system                 etcd-minikube                                100m (2%)     0 (0%)      100Mi (3%)       0 (0%)         16m
  kube-system                 kube-apiserver-minikube                      250m (6%)     0 (0%)      0 (0%)           0 (0%)         16m
  kube-system                 kube-controller-manager-minikube             200m (5%)     0 (0%)      0 (0%)           0 (0%)         16m
  kube-system                 kube-proxy-m4zrn                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         15m
  kube-system                 kube-scheduler-minikube                      100m (2%)     0 (0%)      0 (0%)           0 (0%)         16m
  kube-system                 storage-provisioner                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         14m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                950m (23%)   0 (0%)
  memory             330Mi (11%)  340Mi (11%)
  ephemeral-storage  0 (0%)       0 (0%)
  hugepages-2Mi      0 (0%)       0 (0%)
Events:
  Type    Reason          Age   From             Message
  ----    ------          ----  ----             -------
  Normal  RegisteredNode  15m   node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +0.000005]  <TASK>
[  +0.000013]  dump_stack_lvl+0x4c/0x70
[  +0.000016]  dump_stack+0x14/0x20
[  +0.000004]  warn_alloc+0x10d/0x180
[  +0.000014]  ? __alloc_pages_direct_compact+0x19a/0x290
[  +0.000005]  __alloc_pages_slowpath.constprop.0+0xde2/0xee0
[  +0.000005]  ? kmalloc_trace+0x2e/0xa0
[  +0.000010]  __alloc_pages+0x30c/0x340
[  +0.000005]  vmbus_alloc_ring+0x79/0xd0
[  +0.000003]  ? __pfx_hvs_channel_cb+0x10/0x10
[  +0.000042]  vmbus_open+0x29/0x80
[  +0.000004]  ? vsock_assign_transport+0x1e4/0x260
[  +0.000007]  hvs_probe+0x27f/0x4b0
[  +0.000004]  vmbus_probe+0x42/0xa0
[  +0.000005]  really_probe+0x1a7/0x3f0
[  +0.000012]  ? __pfx___device_attach_driver+0x10/0x10
[  +0.000004]  __driver_probe_device+0x7d/0x170
[  +0.000003]  driver_probe_device+0x24/0xa0
[  +0.000003]  __device_attach_driver+0x98/0x120
[  +0.000004]  bus_for_each_drv+0x8b/0xe0
[  +0.000005]  __device_attach+0xb6/0x1c0
[  +0.000004]  device_initial_probe+0x17/0x20
[  +0.000003]  bus_probe_device+0x9d/0xa0
[  +0.000003]  device_add+0x694/0x8a0
[  +0.000004]  ? hrtimer_init+0x2b/0x80
[  +0.000013]  device_register+0x1e/0x30
[  +0.000004]  vmbus_device_register+0x87/0x130
[  +0.000003]  vmbus_add_channel_work+0x146/0x1d0
[  +0.000005]  process_one_work+0x18e/0x3a0
[  +0.000011]  worker_thread+0x285/0x3c0
[  +0.000005]  ? __pfx_worker_thread+0x10/0x10
[  +0.000004]  kthread+0xf6/0x130
[  +0.000006]  ? __pfx_kthread+0x10/0x10
[  +0.000004]  ret_from_fork+0x41/0x60
[  +0.000009]  ? __pfx_kthread+0x10/0x10
[  +0.000040]  ret_from_fork_asm+0x1b/0x30
[  +0.000012]  </TASK>
[  +0.000003] Mem-Info:
[  +0.000002] active_anon:26535 inactive_anon:228191 isolated_anon:0
               active_file:204110 inactive_file:167877 isolated_file:0
               unevictable:0 dirty:205 writeback:1447
               slab_reclaimable:16646 slab_unreclaimable:14717
               mapped:82314 shmem:1278 pagetables:4560
               sec_pagetables:0 bounce:0
               kernel_misc_reclaimable:0
               free:10001 free_pcp:325 free_cma:0
[  +0.000006] Node 0 active_anon:106140kB inactive_anon:912764kB active_file:816440kB inactive_file:671508kB unevictable:0kB isolated(anon):0kB isolated(file):0kB mapped:329256kB dirty:820kB writeback:5788kB shmem:5112kB shmem_thp:0kB shmem_pmdmapped:0kB anon_thp:0kB writeback_tmp:0kB kernel_stack:9272kB pagetables:18240kB sec_pagetables:0kB all_unreclaimable? no
[  +0.000006] Node 0 DMA32 free:40004kB boost:0kB min:22528kB low:28160kB high:33792kB reserved_highatomic:0KB active_anon:106140kB inactive_anon:911788kB active_file:816688kB inactive_file:671360kB unevictable:0kB writepending:6608kB present:3084924kB managed:2919556kB mlocked:0kB bounce:0kB free_pcp:1300kB local_pcp:8kB free_cma:0kB
[  +0.000008] lowmem_reserve[]: 0 0 0 0
[  +0.000005] Node 0 DMA32: 1159*4kB (UME) 601*8kB (UME) 1214*16kB (UME) 229*32kB (UME) 27*64kB (UME) 10*128kB (UM) 2*256kB (UM) 0*512kB 0*1024kB 0*2048kB 0*4096kB = 39716kB
[  +0.000023] Node 0 hugepages_total=0 hugepages_free=0 hugepages_surp=0 hugepages_size=2048kB
[  +0.000004] 375363 total pagecache pages
[  +0.000001] 2102 pages in swap cache
[  +0.000001] Free swap  = 805752kB
[  +0.000033] Total swap = 1048576kB
[  +0.000001] 771231 pages RAM
[  +0.000001] 0 pages HighMem/MovableOnly
[  +0.000001] 41342 pages reserved
[  +0.000001] 0 pages hwpoisoned
[Feb 9 18:33] WSL (162) ERROR: CheckConnection: getaddrinfo() failed: -5


==> etcd [3ad6fc9caff9] <==
{"level":"warn","ts":"2026-02-09T18:49:06.470561Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"991.241194ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/statefulsets\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2026-02-09T18:49:06.470618Z","caller":"traceutil/trace.go:172","msg":"trace[373179298] range","detail":"{range_begin:/registry/statefulsets; range_end:; response_count:0; response_revision:1321; }","duration":"991.302541ms","start":"2026-02-09T18:49:05.479305Z","end":"2026-02-09T18:49:06.470608Z","steps":["trace[373179298] 'agreement among raft nodes before linearized reading'  (duration: 991.177347ms)"],"step_count":1}
{"level":"warn","ts":"2026-02-09T18:49:06.470660Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2026-02-09T18:49:05.479284Z","time spent":"991.362691ms","remote":"127.0.0.1:32962","response type":"/etcdserverpb.KV/Range","request count":0,"request size":26,"response count":0,"response size":29,"request content":"key:\"/registry/statefulsets\" limit:1 "}
{"level":"info","ts":"2026-02-09T18:49:06.656341Z","caller":"traceutil/trace.go:172","msg":"trace[718585569] linearizableReadLoop","detail":"{readStateIndex:1535; appliedIndex:1535; }","duration":"184.721996ms","start":"2026-02-09T18:49:06.471596Z","end":"2026-02-09T18:49:06.656318Z","steps":["trace[718585569] 'read index received'  (duration: 184.702581ms)","trace[718585569] 'applied index is now lower than readState.Index'  (duration: 8.407¬µs)"],"step_count":2}
{"level":"warn","ts":"2026-02-09T18:49:06.973338Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"501.728118ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2026-02-09T18:49:06.973423Z","caller":"traceutil/trace.go:172","msg":"trace[1856008156] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1321; }","duration":"501.809979ms","start":"2026-02-09T18:49:06.471587Z","end":"2026-02-09T18:49:06.973397Z","steps":["trace[1856008156] 'agreement among raft nodes before linearized reading'  (duration: 184.833881ms)","trace[1856008156] 'range keys from in-memory index tree'  (duration: 316.872821ms)"],"step_count":2}
{"level":"warn","ts":"2026-02-09T18:49:07.150638Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"174.59408ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2026-02-09T18:49:07.150707Z","caller":"traceutil/trace.go:172","msg":"trace[721283371] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1321; }","duration":"174.669826ms","start":"2026-02-09T18:49:06.976024Z","end":"2026-02-09T18:49:07.150694Z","steps":["trace[721283371] 'range keys from in-memory index tree'  (duration: 174.577669ms)"],"step_count":1}
{"level":"warn","ts":"2026-02-09T18:49:07.150640Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"494.18135ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128043241971802797 > lease_revoke:<id:70cc9c43acbe6265>","response":"size:29"}
{"level":"info","ts":"2026-02-09T18:49:07.150859Z","caller":"traceutil/trace.go:172","msg":"trace[1060942609] linearizableReadLoop","detail":"{readStateIndex:1536; appliedIndex:1535; }","duration":"494.450618ms","start":"2026-02-09T18:49:06.656398Z","end":"2026-02-09T18:49:07.150849Z","steps":["trace[1060942609] 'read index received'  (duration: 227.250989ms)","trace[1060942609] 'applied index is now lower than readState.Index'  (duration: 267.198028ms)"],"step_count":2}
{"level":"warn","ts":"2026-02-09T18:49:07.150969Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2026-02-09T18:49:06.474162Z","time spent":"676.804009ms","remote":"127.0.0.1:60322","response type":"/etcdserverpb.Lease/LeaseGrant","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"warn","ts":"2026-02-09T18:49:07.151467Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"173.381446ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2026-02-09T18:49:07.151505Z","caller":"traceutil/trace.go:172","msg":"trace[1035165209] range","detail":"{range_begin:/registry/namespaces; range_end:; response_count:0; response_revision:1322; }","duration":"173.462796ms","start":"2026-02-09T18:49:06.978031Z","end":"2026-02-09T18:49:07.151494Z","steps":["trace[1035165209] 'agreement among raft nodes before linearized reading'  (duration: 173.360734ms)"],"step_count":1}
{"level":"warn","ts":"2026-02-09T18:49:07.151669Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"677.429477ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2026-02-09T18:49:07.151701Z","caller":"traceutil/trace.go:172","msg":"trace[848288702] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1321; }","duration":"677.462997ms","start":"2026-02-09T18:49:06.474233Z","end":"2026-02-09T18:49:07.151696Z","steps":["trace[848288702] 'agreement among raft nodes before linearized reading'  (duration: 676.859432ms)"],"step_count":1}
{"level":"warn","ts":"2026-02-09T18:49:07.151729Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2026-02-09T18:49:06.474219Z","time spent":"677.49712ms","remote":"127.0.0.1:60266","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2026-02-09T18:49:07.151872Z","caller":"traceutil/trace.go:172","msg":"trace[473765260] transaction","detail":"{read_only:false; response_revision:1322; number_of_response:1; }","duration":"672.788445ms","start":"2026-02-09T18:49:06.479077Z","end":"2026-02-09T18:49:07.151865Z","steps":["trace[473765260] 'process raft request'  (duration: 671.776332ms)"],"step_count":1}
{"level":"warn","ts":"2026-02-09T18:49:07.152034Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2026-02-09T18:49:06.479058Z","time spent":"672.897814ms","remote":"127.0.0.1:60558","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:1318 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2026-02-09T18:49:07.155979Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"677.073166ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/leases/ingress-nginx/ingress-nginx-leader\" limit:1 ","response":"range_response_count:1 size:501"}
{"level":"info","ts":"2026-02-09T18:49:07.156046Z","caller":"traceutil/trace.go:172","msg":"trace[1798122250] range","detail":"{range_begin:/registry/leases/ingress-nginx/ingress-nginx-leader; range_end:; response_count:1; response_revision:1321; }","duration":"677.151413ms","start":"2026-02-09T18:49:06.478882Z","end":"2026-02-09T18:49:07.156034Z","steps":["trace[1798122250] 'agreement among raft nodes before linearized reading'  (duration: 672.046724ms)"],"step_count":1}
{"level":"warn","ts":"2026-02-09T18:49:07.156080Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2026-02-09T18:49:06.478867Z","time spent":"677.202646ms","remote":"127.0.0.1:60732","response type":"/etcdserverpb.KV/Range","request count":0,"request size":55,"response count":1,"response size":525,"request content":"key:\"/registry/leases/ingress-nginx/ingress-nginx-leader\" limit:1 "}
{"level":"info","ts":"2026-02-09T18:49:07.349420Z","caller":"traceutil/trace.go:172","msg":"trace[17189090] linearizableReadLoop","detail":"{readStateIndex:1538; appliedIndex:1538; }","duration":"185.457904ms","start":"2026-02-09T18:49:07.163929Z","end":"2026-02-09T18:49:07.349387Z","steps":["trace[17189090] 'read index received'  (duration: 185.446697ms)","trace[17189090] 'applied index is now lower than readState.Index'  (duration: 9.806¬µs)"],"step_count":2}
{"level":"warn","ts":"2026-02-09T18:49:07.874944Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"710.999424ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2026-02-09T18:49:07.875019Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"524.463137ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128043241971802805 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:1317 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128043241971802799 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:16"}
{"level":"info","ts":"2026-02-09T18:49:07.875047Z","caller":"traceutil/trace.go:172","msg":"trace[1488020920] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1322; }","duration":"711.099692ms","start":"2026-02-09T18:49:07.163924Z","end":"2026-02-09T18:49:07.875024Z","steps":["trace[1488020920] 'agreement among raft nodes before linearized reading'  (duration: 185.997031ms)","trace[1488020920] 'range keys from in-memory index tree'  (duration: 524.964091ms)"],"step_count":2}
{"level":"warn","ts":"2026-02-09T18:49:07.875094Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2026-02-09T18:49:07.163904Z","time spent":"711.180023ms","remote":"127.0.0.1:60282","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2026-02-09T18:49:07.875140Z","caller":"traceutil/trace.go:172","msg":"trace[1852616047] transaction","detail":"{read_only:false; response_revision:1323; number_of_response:1; }","duration":"718.181469ms","start":"2026-02-09T18:49:07.156943Z","end":"2026-02-09T18:49:07.875125Z","steps":["trace[1852616047] 'process raft request'  (duration: 193.550901ms)","trace[1852616047] 'compare'  (duration: 524.234463ms)"],"step_count":2}
{"level":"info","ts":"2026-02-09T18:49:07.875187Z","caller":"traceutil/trace.go:172","msg":"trace[175420308] transaction","detail":"{read_only:false; response_revision:1324; number_of_response:1; }","duration":"407.820975ms","start":"2026-02-09T18:49:07.467356Z","end":"2026-02-09T18:49:07.875177Z","steps":["trace[175420308] 'process raft request'  (duration: 407.742763ms)"],"step_count":1}
{"level":"warn","ts":"2026-02-09T18:49:07.875220Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2026-02-09T18:49:07.156915Z","time spent":"718.262781ms","remote":"127.0.0.1:60322","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":116,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:1317 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128043241971802799 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >"}
{"level":"warn","ts":"2026-02-09T18:49:07.875250Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2026-02-09T18:49:07.467303Z","time spent":"407.911774ms","remote":"127.0.0.1:60732","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":673,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:1319 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"info","ts":"2026-02-09T18:49:07.875409Z","caller":"traceutil/trace.go:172","msg":"trace[523914721] linearizableReadLoop","detail":"{readStateIndex:1539; appliedIndex:1538; }","duration":"524.363831ms","start":"2026-02-09T18:49:07.351035Z","end":"2026-02-09T18:49:07.875399Z","steps":["trace[523914721] 'read index received'  (duration: 59.636¬µs)","trace[523914721] 'applied index is now lower than readState.Index'  (duration: 524.303009ms)"],"step_count":2}
{"level":"warn","ts":"2026-02-09T18:49:07.875493Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"524.456282ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumeclaims\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2026-02-09T18:49:07.875524Z","caller":"traceutil/trace.go:172","msg":"trace[680204605] range","detail":"{range_begin:/registry/persistentvolumeclaims; range_end:; response_count:0; response_revision:1324; }","duration":"524.489315ms","start":"2026-02-09T18:49:07.351026Z","end":"2026-02-09T18:49:07.875515Z","steps":["trace[680204605] 'agreement among raft nodes before linearized reading'  (duration: 524.418717ms)"],"step_count":1}
{"level":"warn","ts":"2026-02-09T18:49:07.875588Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2026-02-09T18:49:07.350996Z","time spent":"524.55176ms","remote":"127.0.0.1:60528","response type":"/etcdserverpb.KV/Range","request count":0,"request size":36,"response count":0,"response size":29,"request content":"key:\"/registry/persistentvolumeclaims\" limit:1 "}
{"level":"info","ts":"2026-02-09T18:49:09.360048Z","caller":"traceutil/trace.go:172","msg":"trace[1681765386] transaction","detail":"{read_only:false; response_revision:1325; number_of_response:1; }","duration":"192.019427ms","start":"2026-02-09T18:49:09.168008Z","end":"2026-02-09T18:49:09.360028Z","steps":["trace[1681765386] 'process raft request'  (duration: 191.847126ms)"],"step_count":1}
{"level":"warn","ts":"2026-02-09T18:49:09.746973Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"282.800985ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/limitranges/\" range_end:\"/registry/limitranges0\" limit:10000 revision:1320 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2026-02-09T18:49:09.747097Z","caller":"traceutil/trace.go:172","msg":"trace[1493372313] range","detail":"{range_begin:/registry/limitranges/; range_end:/registry/limitranges0; response_count:0; response_revision:1325; }","duration":"282.927914ms","start":"2026-02-09T18:49:09.464141Z","end":"2026-02-09T18:49:09.747069Z","steps":["trace[1493372313] 'range keys from in-memory index tree'  (duration: 282.682955ms)"],"step_count":1}
{"level":"warn","ts":"2026-02-09T18:49:11.255789Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"185.942503ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128043241971802823 > lease_revoke:<id:70cc9c43acbe6298>","response":"size:29"}
{"level":"info","ts":"2026-02-09T18:49:11.256105Z","caller":"traceutil/trace.go:172","msg":"trace[682307453] linearizableReadLoop","detail":"{readStateIndex:1542; appliedIndex:1541; }","duration":"184.939431ms","start":"2026-02-09T18:49:11.071088Z","end":"2026-02-09T18:49:11.256027Z","steps":["trace[682307453] 'read index received'  (duration: 27.278¬µs)","trace[682307453] 'applied index is now lower than readState.Index'  (duration: 184.9081ms)"],"step_count":2}
{"level":"warn","ts":"2026-02-09T18:49:11.256260Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"185.182169ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2026-02-09T18:49:11.256333Z","caller":"traceutil/trace.go:172","msg":"trace[987391714] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1325; }","duration":"185.263015ms","start":"2026-02-09T18:49:11.071047Z","end":"2026-02-09T18:49:11.256310Z","steps":["trace[987391714] 'agreement among raft nodes before linearized reading'  (duration: 185.124153ms)"],"step_count":1}
{"level":"info","ts":"2026-02-09T18:49:13.632505Z","caller":"traceutil/trace.go:172","msg":"trace[1688146635] transaction","detail":"{read_only:false; response_revision:1327; number_of_response:1; }","duration":"110.58671ms","start":"2026-02-09T18:49:13.521900Z","end":"2026-02-09T18:49:13.632486Z","steps":["trace[1688146635] 'process raft request'  (duration: 110.364103ms)"],"step_count":1}
{"level":"warn","ts":"2026-02-09T18:49:13.871200Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"148.000563ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/minions\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2026-02-09T18:49:13.871328Z","caller":"traceutil/trace.go:172","msg":"trace[17914145] range","detail":"{range_begin:/registry/minions; range_end:; response_count:0; response_revision:1327; }","duration":"148.125667ms","start":"2026-02-09T18:49:13.723170Z","end":"2026-02-09T18:49:13.871296Z","steps":["trace[17914145] 'range keys from in-memory index tree'  (duration: 147.88706ms)"],"step_count":1}
{"level":"info","ts":"2026-02-09T18:49:14.773408Z","caller":"traceutil/trace.go:172","msg":"trace[556625288] transaction","detail":"{read_only:false; response_revision:1328; number_of_response:1; }","duration":"109.093245ms","start":"2026-02-09T18:49:14.664290Z","end":"2026-02-09T18:49:14.773383Z","steps":["trace[556625288] 'process raft request'  (duration: 108.91285ms)"],"step_count":1}
{"level":"warn","ts":"2026-02-09T18:49:15.115562Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"106.459172ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2026-02-09T18:49:15.115644Z","caller":"traceutil/trace.go:172","msg":"trace[1320595010] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1328; }","duration":"106.549432ms","start":"2026-02-09T18:49:15.006986Z","end":"2026-02-09T18:49:15.115629Z","steps":["trace[1320595010] 'range keys from in-memory index tree'  (duration: 106.372287ms)"],"step_count":1}
{"level":"warn","ts":"2026-02-09T18:49:15.115597Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"149.217766ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2026-02-09T18:49:15.115736Z","caller":"traceutil/trace.go:172","msg":"trace[1856877255] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1328; }","duration":"149.375797ms","start":"2026-02-09T18:49:14.964251Z","end":"2026-02-09T18:49:15.115721Z","steps":["trace[1856877255] 'range keys from in-memory index tree'  (duration: 149.141472ms)"],"step_count":1}
{"level":"info","ts":"2026-02-09T18:49:15.374238Z","caller":"traceutil/trace.go:172","msg":"trace[663890165] linearizableReadLoop","detail":"{readStateIndex:1545; appliedIndex:1545; }","duration":"143.185395ms","start":"2026-02-09T18:49:15.231028Z","end":"2026-02-09T18:49:15.374214Z","steps":["trace[663890165] 'read index received'  (duration: 143.158607ms)","trace[663890165] 'applied index is now lower than readState.Index'  (duration: 9.296¬µs)"],"step_count":2}
{"level":"info","ts":"2026-02-09T18:49:15.374394Z","caller":"traceutil/trace.go:172","msg":"trace[2125501852] transaction","detail":"{read_only:false; response_revision:1329; number_of_response:1; }","duration":"216.092414ms","start":"2026-02-09T18:49:15.158288Z","end":"2026-02-09T18:49:15.374380Z","steps":["trace[2125501852] 'process raft request'  (duration: 215.923088ms)"],"step_count":1}
{"level":"warn","ts":"2026-02-09T18:49:15.374486Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"143.455776ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.49.2\" limit:1 ","response":"range_response_count:1 size:131"}
{"level":"info","ts":"2026-02-09T18:49:15.374525Z","caller":"traceutil/trace.go:172","msg":"trace[1653720773] range","detail":"{range_begin:/registry/masterleases/192.168.49.2; range_end:; response_count:1; response_revision:1329; }","duration":"143.518948ms","start":"2026-02-09T18:49:15.230997Z","end":"2026-02-09T18:49:15.374516Z","steps":["trace[1653720773] 'agreement among raft nodes before linearized reading'  (duration: 143.345524ms)"],"step_count":1}
{"level":"warn","ts":"2026-02-09T18:49:15.685436Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"191.377489ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128043241971802844 username:\"kube-apiserver-etcd-client\" auth_revision:1 > lease_grant:<ttl:15-second id:70cc9c43acbe62db>","response":"size:41"}
{"level":"warn","ts":"2026-02-09T18:49:15.685779Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2026-02-09T18:49:15.376114Z","time spent":"309.655043ms","remote":"127.0.0.1:60322","response type":"/etcdserverpb.Lease/LeaseGrant","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"info","ts":"2026-02-09T18:49:15.960651Z","caller":"traceutil/trace.go:172","msg":"trace[1518839582] transaction","detail":"{read_only:false; response_revision:1331; number_of_response:1; }","duration":"142.946143ms","start":"2026-02-09T18:49:15.817680Z","end":"2026-02-09T18:49:15.960626Z","steps":["trace[1518839582] 'process raft request'  (duration: 106.011115ms)","trace[1518839582] 'compare'  (duration: 36.796878ms)"],"step_count":2}
{"level":"warn","ts":"2026-02-09T18:49:15.960699Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"156.948108ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/endpointslices/default/kubernetes\" limit:1 ","response":"range_response_count:1 size:474"}
{"level":"info","ts":"2026-02-09T18:49:15.960858Z","caller":"traceutil/trace.go:172","msg":"trace[1621282543] range","detail":"{range_begin:/registry/endpointslices/default/kubernetes; range_end:; response_count:1; response_revision:1330; }","duration":"157.123365ms","start":"2026-02-09T18:49:15.803722Z","end":"2026-02-09T18:49:15.960846Z","steps":["trace[1621282543] 'range keys from in-memory index tree'  (duration: 156.772952ms)"],"step_count":1}
{"level":"info","ts":"2026-02-09T18:49:18.393883Z","caller":"traceutil/trace.go:172","msg":"trace[1858426287] transaction","detail":"{read_only:false; response_revision:1333; number_of_response:1; }","duration":"336.568157ms","start":"2026-02-09T18:49:18.057291Z","end":"2026-02-09T18:49:18.393859Z","steps":["trace[1858426287] 'process raft request'  (duration: 287.973423ms)","trace[1858426287] 'compare'  (duration: 48.380022ms)"],"step_count":2}
{"level":"warn","ts":"2026-02-09T18:49:18.394078Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2026-02-09T18:49:18.057263Z","time spent":"336.702094ms","remote":"127.0.0.1:60558","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:1331 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}


==> kernel <==
 18:49:21 up 52 min,  0 user,  load average: 5.49, 5.84, 6.17
Linux minikube 6.6.114.1-microsoft-standard-WSL2 #1 SMP PREEMPT_DYNAMIC Mon Dec  1 20:46:23 UTC 2025 x86_64 GNU/Linux
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"


==> kube-apiserver [c7fa8edd7d6e] <==
{"level":"warn","ts":"2026-02-09T18:34:39.077169Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0007a05a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:34:44.903016Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0005745a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:34:48.828708Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0005745a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:34:51.841859Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0005745a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:34:53.842928Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0007a05a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:35:29.297248Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0005745a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:36:15.271769Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0005745a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
I0209 18:37:16.697163       1 controller.go:667] quota admission added evaluator for: namespaces
I0209 18:37:20.415388       1 alloc.go:329] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller" clusterIPs={"IPv4":"10.108.199.186"}
I0209 18:37:21.751474       1 alloc.go:329] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller-admission" clusterIPs={"IPv4":"10.104.203.55"}
I0209 18:37:22.382931       1 controller.go:667] quota admission added evaluator for: jobs.batch
{"level":"warn","ts":"2026-02-09T18:38:07.503583Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0005745a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:38:09.505719Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0007a05a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
I0209 18:43:12.665675       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
{"level":"warn","ts":"2026-02-09T18:45:29.987124Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0007a05a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
I0209 18:45:33.230228       1 stats.go:119] "Error getting keys" err="Timeout: Too large resource version: 1162, current: 1161"
{"level":"warn","ts":"2026-02-09T18:45:33.327026Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0005745a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:45:37.062767Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0005745a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:45:41.509666Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0007a05a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:45:47.131589Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0005745a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:45:49.450253Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0005745a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:46:08.218196Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0007a05a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:46:20.800789Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0007a05a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:46:22.818442Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0005745a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:46:24.819254Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0007a05a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:46:24.884872Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc000faa5a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Txn","attempt":0,"error":"rpc error: code = Unavailable desc = etcdserver: request timed out"}
E0209 18:46:27.805295       1 writers.go:123] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0209 18:46:28.171280       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
{"level":"warn","ts":"2026-02-09T18:46:28.377668Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0007a05a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
E0209 18:46:28.520020       1 writers.go:136] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0209 18:46:28.520362       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="1.684882611s" method="PUT" path="/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath" result=null
E0209 18:46:29.485783       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 2.472270379s, panicked: false, err: etcdserver: request timed out, panic-reason: <nil>" logger="UnhandledError"
{"level":"warn","ts":"2026-02-09T18:46:31.970386Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0005745a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:46:33.455842Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0005745a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:46:33.971083Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0007a05a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:46:36.035001Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0005745a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:46:43.357923Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0007a05a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:46:43.453434Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0005745a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:46:47.116741Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0005b34a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Txn","attempt":0,"error":"rpc error: code = Unavailable desc = etcdserver: request timed out"}
E0209 18:46:47.117025       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
{"level":"warn","ts":"2026-02-09T18:46:47.449811Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc001343a40/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = Unavailable desc = etcdserver: request timed out"}
{"level":"warn","ts":"2026-02-09T18:46:57.967848Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0007a05a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:47:18.467711Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0007a05a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:47:20.487146Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0005745a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:47:17.013655Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0007a05a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:47:29.585004Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0007a05a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:47:36.841415Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0005745a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:47:55.096782Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0005745a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:47:57.098717Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0007a05a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:47:58.358229Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc00177bc20/127.0.0.1:2379","method":"/etcdserverpb.KV/Txn","attempt":0,"error":"rpc error: code = Unavailable desc = etcdserver: request timed out"}
E0209 18:47:58.358594       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
{"level":"warn","ts":"2026-02-09T18:48:32.105316Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0005745a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:48:33.505706Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0005745a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:48:34.106112Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0007a05a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:49:00.001787Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0007a05a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:49:02.021947Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0005745a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:49:03.504919Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0005745a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:49:04.022614Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0007a05a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2026-02-09T18:49:04.894004Z","logger":"etcd-client","caller":"v3@v3.6.5/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc00177bc20/127.0.0.1:2379","method":"/etcdserverpb.KV/Txn","attempt":0,"error":"rpc error: code = Unavailable desc = etcdserver: request timed out"}
E0209 18:49:04.894818       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"


==> kube-apiserver [cb5a0c27b5ff] <==
I0209 18:32:23.932727       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0209 18:32:23.978907       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0209 18:32:23.942060       1 controller.go:127] "Starting legacy_token_tracking_controller"
I0209 18:32:23.978987       1 shared_informer.go:370] "Waiting for caches to sync"
I0209 18:32:23.932987       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0209 18:32:24.006981       1 controller.go:113] "Deleting old lease on startup" lease="kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii"
I0209 18:32:23.892361       1 remote_available_controller.go:425] Starting RemoteAvailability controller
I0209 18:32:24.012757       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0209 18:32:23.892745       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0209 18:32:24.013799       1 gc_controller.go:85] "Starting apiserver lease garbage collector"
I0209 18:32:24.013835       1 shared_informer.go:370] "Waiting for caches to sync"
I0209 18:32:24.035811       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0209 18:32:24.086468       1 customresource_discovery_controller.go:294] Starting DiscoveryController
I0209 18:32:24.095780       1 cluster_authentication_trust_controller.go:459] Starting cluster_authentication_trust_controller controller
I0209 18:32:24.095869       1 shared_informer.go:370] "Waiting for caches to sync"
I0209 18:32:24.097471       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0209 18:32:24.109191       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0209 18:32:24.109361       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0209 18:32:24.110435       1 controller.go:90] Starting OpenAPI V3 controller
I0209 18:32:24.110522       1 naming_controller.go:305] Starting NamingConditionController
I0209 18:32:24.110588       1 nonstructuralschema_controller.go:202] Starting NonStructuralSchemaConditionController
I0209 18:32:24.110654       1 apiapproval_controller.go:196] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0209 18:32:24.110681       1 crd_finalizer.go:273] Starting CRDFinalizer
I0209 18:32:24.118733       1 controller.go:142] Starting OpenAPI controller
I0209 18:32:24.176186       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0209 18:32:24.176242       1 shared_informer.go:349] "Waiting for caches to sync" controller="crd-autoregister"
I0209 18:32:24.176291       1 controller.go:78] Starting OpenAPI AggregationController
I0209 18:32:24.176404       1 repairip.go:210] Starting ipallocator-repair-controller
I0209 18:32:24.176420       1 shared_informer.go:349] "Waiting for caches to sync" controller="ipallocator-repair-controller"
I0209 18:32:24.187556       1 default_servicecidr_controller.go:110] Starting kubernetes-service-cidr-controller
I0209 18:32:24.187622       1 shared_informer.go:349] "Waiting for caches to sync" controller="kubernetes-service-cidr-controller"
I0209 18:32:24.349920       1 shared_informer.go:377] "Caches are synced"
I0209 18:32:24.372978       1 shared_informer.go:377] "Caches are synced"
I0209 18:32:24.377771       1 shared_informer.go:356] "Caches are synced" controller="crd-autoregister"
I0209 18:32:24.381285       1 aggregator.go:187] initial CRD sync complete...
I0209 18:32:24.381346       1 autoregister_controller.go:144] Starting autoregister controller
I0209 18:32:24.381361       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0209 18:32:24.394970       1 shared_informer.go:377] "Caches are synced"
I0209 18:32:24.419421       1 shared_informer.go:377] "Caches are synced"
I0209 18:32:24.420293       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0209 18:32:24.420631       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0209 18:32:24.421076       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0209 18:32:24.449371       1 shared_informer.go:356] "Caches are synced" controller="node_authorizer"
I0209 18:32:24.449647       1 policy_source.go:248] refreshing policies
I0209 18:32:24.450498       1 cache.go:39] Caches are synced for LocalAvailability controller
I0209 18:32:24.504318       1 shared_informer.go:356] "Caches are synced" controller="kubernetes-service-cidr-controller"
I0209 18:32:24.504354       1 controller.go:667] quota admission added evaluator for: namespaces
I0209 18:32:24.504433       1 default_servicecidr_controller.go:169] Creating default ServiceCIDR with CIDRs: [10.96.0.0/12]
I0209 18:32:24.505883       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0209 18:32:24.506299       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0209 18:32:24.507421       1 shared_informer.go:356] "Caches are synced" controller="ipallocator-repair-controller"
I0209 18:32:24.507420       1 cache.go:39] Caches are synced for autoregister controller
E0209 18:32:24.604700       1 controller.go:156] "Error while syncing ConfigMap" err="namespaces \"kube-system\" not found" logger="UnhandledError" configmap="kube-system/kube-apiserver-legacy-service-account-token-tracking"
I0209 18:32:24.866211       1 cidrallocator.go:302] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I0209 18:32:24.867289       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I0209 18:32:24.867489       1 default_servicecidr_controller.go:231] Setting default ServiceCIDR condition Ready to True
I0209 18:32:25.350906       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0209 18:32:25.797155       1 storage_scheduling.go:123] created PriorityClass system-node-critical with value 2000001000
I0209 18:32:27.602755       1 storage_scheduling.go:123] created PriorityClass system-cluster-critical with value 2000000000
I0209 18:32:27.602863       1 storage_scheduling.go:139] all system priority classes are created successfully or already exist.


==> kube-controller-manager [35cbf93a2440] <==
I0209 18:33:50.106801       1 graph_builder.go:385] "Running" component="GraphBuilder"
I0209 18:33:50.172408       1 cleaner.go:83] "Starting CSR cleaner controller"
I0209 18:33:50.204764       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:50.204949       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:50.204970       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:50.205035       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:50.205264       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:50.205412       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0209 18:33:50.204975       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:50.216321       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:50.216437       1 range_allocator.go:177] "Sending events to api server"
I0209 18:33:50.216524       1 range_allocator.go:181] "Starting range CIDR allocator"
I0209 18:33:50.216551       1 shared_informer.go:370] "Waiting for caches to sync"
I0209 18:33:50.216562       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:50.356722       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:50.640941       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:50.641041       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:50.641072       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:50.641588       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:50.641695       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:50.641836       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:50.699735       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:50.699931       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:50.716182       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:50.717000       1 range_allocator.go:433] "Set node PodCIDR" node="minikube" podCIDRs=["10.244.0.0/24"]
I0209 18:33:50.719452       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:50.719561       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:50.719783       1 node_lifecycle_controller.go:1234] "Initializing eviction metric for zone" zone=""
I0209 18:33:50.720041       1 node_lifecycle_controller.go:886] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0209 18:33:50.720293       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:50.720370       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:50.720508       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:50.720658       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:50.808307       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:50.808387       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:50.808523       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:50.808335       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:50.968368       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:50.968709       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:51.048634       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:51.126519       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:51.126931       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:51.126980       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:51.127073       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:51.127092       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:51.206760       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:51.214003       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:51.300989       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:51.339281       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:51.339678       1 node_lifecycle_controller.go:1038] "Controller detected that all Nodes are not-Ready. Entering master disruption mode"
I0209 18:33:51.341955       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:51.342030       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:51.342105       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:51.351433       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:51.531354       1 shared_informer.go:370] "Waiting for caches to sync"
I0209 18:33:52.507583       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:52.507675       1 garbagecollector.go:166] "Garbage collector: all resource monitors have synced"
I0209 18:33:52.507694       1 garbagecollector.go:169] "Proceeding to collect garbage"
I0209 18:33:52.531641       1 shared_informer.go:377] "Caches are synced"
I0209 18:33:57.929371       1 node_lifecycle_controller.go:1057] "Controller detected that some Nodes are Ready. Exiting master disruption mode"


==> kube-controller-manager [b6b331fe6327] <==
I0209 18:33:06.762971       1 serving.go:386] Generated self-signed cert in-memory
I0209 18:33:06.782720       1 controllermanager.go:189] "Starting" version="v1.35.0"
I0209 18:33:06.783732       1 controllermanager.go:191] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0209 18:33:06.789481       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0209 18:33:06.789498       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0209 18:33:06.789891       1 secure_serving.go:211] Serving securely on 127.0.0.1:10257
I0209 18:33:06.790111       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E0209 18:33:16.898726       1 controllermanager.go:250] "Error building controller context" err="failed to wait for apiserver being healthy: timed out waiting for the condition: failed to get apiserver /healthz status: an error on the server (\"[+]ping ok\\n[+]log ok\\n[+]etcd ok\\n[+]poststarthook/start-apiserver-admission-initializer ok\\n[+]poststarthook/generic-apiserver-start-informers ok\\n[+]poststarthook/priority-and-fairness-config-consumer ok\\n[+]poststarthook/priority-and-fairness-filter ok\\n[+]poststarthook/storage-object-count-tracker-hook ok\\n[+]poststarthook/start-apiextensions-informers ok\\n[+]poststarthook/start-apiextensions-controllers ok\\n[+]poststarthook/crd-informer-synced ok\\n[+]poststarthook/start-system-namespaces-controller ok\\n[+]poststarthook/start-cluster-authentication-info-controller ok\\n[+]poststarthook/start-kube-apiserver-identity-lease-controller ok\\n[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok\\n[+]poststarthook/start-legacy-token-tracking-controller ok\\n[+]poststarthook/start-service-ip-repair-controllers ok\\n[-]poststarthook/rbac/bootstrap-roles failed: reason withheld\\n[+]poststarthook/scheduling/bootstrap-system-priority-classes ok\\n[+]poststarthook/priority-and-fairness-config-producer ok\\n[+]poststarthook/bootstrap-controller ok\\n[+]poststarthook/start-kubernetes-service-cidr-controller ok\\n[+]poststarthook/aggregator-reload-proxy-client-cert ok\\n[+]poststarthook/start-kube-aggregator-informers ok\\n[+]poststarthook/apiservice-status-local-available-controller ok\\n[+]poststarthook/apiservice-status-remote-available-controller ok\\n[+]poststarthook/apiservice-registration-controller ok\\n[+]poststarthook/apiservice-discovery-controller ok\\n[+]poststarthook/kube-apiserver-autoregistration ok\\n[+]autoregister-completion ok\\n[+]poststarthook/apiservice-openapi-controller ok\\n[+]poststarthook/apiservice-openapiv3-controller ok\\nhealthz check failed\") has prevented the request from succeeding"


==> kube-proxy [47ba7b1cb117] <==
I0209 18:35:22.969266       1 server_linux.go:53] "Using iptables proxy"
I0209 18:35:38.112293       1 shared_informer.go:370] "Waiting for caches to sync"
I0209 18:35:44.011889       1 shared_informer.go:377] "Caches are synced"
I0209 18:35:44.011988       1 server.go:218] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E0209 18:35:44.012208       1 server.go:255] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0209 18:35:48.588477       1 server.go:264] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0209 18:35:48.588603       1 server_linux.go:136] "Using iptables Proxier"
I0209 18:35:48.605955       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0209 18:35:49.801117       1 server.go:529] "Version info" version="v1.35.0"
I0209 18:35:49.802024       1 server.go:531] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0209 18:35:49.813016       1 config.go:200] "Starting service config controller"
I0209 18:35:49.813706       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I0209 18:35:49.814363       1 config.go:106] "Starting endpoint slice config controller"
I0209 18:35:49.814756       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I0209 18:35:49.814371       1 config.go:403] "Starting serviceCIDR config controller"
I0209 18:35:49.815656       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I0209 18:35:49.816283       1 config.go:309] "Starting node config controller"
I0209 18:35:49.816675       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I0209 18:35:49.816931       1 shared_informer.go:356] "Caches are synced" controller="node config"
I0209 18:35:50.316036       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"
I0209 18:35:50.316199       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"
I0209 18:35:50.414476       1 shared_informer.go:356] "Caches are synced" controller="service config"


==> kube-scheduler [ed7d6b75b3c5] <==
I0209 18:32:08.077779       1 serving.go:386] Generated self-signed cert in-memory
W0209 18:32:08.242989       1 authentication.go:397] Error looking up in-cluster authentication configuration: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps/extension-apiserver-authentication": dial tcp 192.168.49.2:8443: connect: connection refused
W0209 18:32:08.243055       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0209 18:32:08.243068       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0209 18:32:08.393885       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.35.0"
I0209 18:32:08.393951       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0209 18:32:08.397623       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0209 18:32:08.397720       1 shared_informer.go:370] "Waiting for caches to sync"
I0209 18:32:08.397839       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0209 18:32:08.398084       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E0209 18:32:24.162923       1 reflector.go:204] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.ResourceSlice"
E0209 18:32:24.163603       1 reflector.go:204] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.Pod"
E0209 18:32:24.164670       1 reflector.go:204] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.VolumeAttachment"
E0209 18:32:24.164941       1 reflector.go:204] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.StatefulSet"
E0209 18:32:24.165086       1 reflector.go:204] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.Namespace"
E0209 18:32:24.166567       1 reflector.go:204] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.CSIDriver"
E0209 18:32:24.228002       1 reflector.go:204] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.PersistentVolumeClaim"
E0209 18:32:24.228002       1 reflector.go:204] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.DeviceClass"
E0209 18:32:24.228330       1 reflector.go:204] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.StorageClass"
E0209 18:32:24.228503       1 reflector.go:204] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.PersistentVolume"
E0209 18:32:24.228591       1 reflector.go:204] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.ReplicaSet"
E0209 18:32:24.229533       1 reflector.go:204] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.PodDisruptionBudget"
E0209 18:32:24.229941       1 reflector.go:204] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.Service"
E0209 18:32:26.462885       1 reflector.go:204] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.CSINode"
E0209 18:32:27.255552       1 reflector.go:204] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.ResourceClaim"
E0209 18:32:27.634858       1 reflector.go:204] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1693" type="*v1.ConfigMap"
E0209 18:32:28.006543       1 reflector.go:204] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.Node"
E0209 18:32:28.517312       1 reflector.go:204] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.CSIStorageCapacity"
E0209 18:32:29.751514       1 reflector.go:204] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.ReplicationController"
E0209 18:32:30.897559       1 reflector.go:204] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.StorageClass"
E0209 18:32:30.956065       1 reflector.go:204] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.Namespace"
E0209 18:32:31.150385       1 reflector.go:204] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.StatefulSet"
E0209 18:32:31.503600       1 reflector.go:204] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.PersistentVolume"
E0209 18:32:31.537034       1 reflector.go:204] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.Pod"
E0209 18:32:31.859505       1 reflector.go:204] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.VolumeAttachment"
E0209 18:32:32.187833       1 reflector.go:204] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.CSIDriver"
E0209 18:32:32.376093       1 reflector.go:204] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.ResourceSlice"
E0209 18:32:34.866566       1 reflector.go:204] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.PersistentVolumeClaim"
E0209 18:32:36.842607       1 reflector.go:204] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.Service"
E0209 18:32:37.949860       1 reflector.go:204] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.ReplicaSet"
E0209 18:32:41.080608       1 reflector.go:204] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.Node"
E0209 18:32:42.501909       1 reflector.go:204] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.ResourceClaim"
E0209 18:32:44.057705       1 reflector.go:204] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.PodDisruptionBudget"
E0209 18:32:46.384978       1 reflector.go:204] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.PersistentVolume"
E0209 18:32:46.880215       1 reflector.go:204] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.CSINode"
E0209 18:32:47.806815       1 reflector.go:204] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.CSIStorageCapacity"
E0209 18:32:48.499700       1 reflector.go:204] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.DeviceClass"
E0209 18:32:49.927264       1 reflector.go:204] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.Service"
E0209 18:32:50.224536       1 reflector.go:204] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.ResourceSlice"
E0209 18:32:50.403980       1 reflector.go:204] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1693" type="*v1.ConfigMap"
E0209 18:32:50.674825       1 reflector.go:204] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.StatefulSet"
E0209 18:32:51.200979       1 reflector.go:204] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.VolumeAttachment"
E0209 18:32:51.978387       1 reflector.go:204] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.StorageClass"
E0209 18:32:52.226247       1 reflector.go:204] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.Pod"
E0209 18:32:52.798444       1 reflector.go:204] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.PersistentVolumeClaim"
I0209 18:32:57.025195       1 reflector.go:578] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.CSIDriver" err="very short watch: k8s.io/client-go/informers/factory.go:161: Unexpected watch close - watch lasted less than a second and no items received"
E0209 18:33:22.819974       1 reflector.go:204] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1693" type="*v1.ConfigMap"
I0209 18:34:04.782558       1 shared_informer.go:377] "Caches are synced"


==> kubelet <==
Feb 09 18:40:52 minikube kubelet[2431]: E0209 18:40:52.263959    2431 prober_manager.go:209] "Readiness probe already exists for container" pod="kube-system/coredns-7d764666f9-99hn5" containerName="coredns"
Feb 09 18:41:24 minikube kubelet[2431]: E0209 18:41:24.190252    2431 prober_manager.go:209] "Readiness probe already exists for container" pod="kube-system/coredns-7d764666f9-4nzzl" containerName="coredns"
Feb 09 18:41:28 minikube kubelet[2431]: E0209 18:41:28.137527    2431 prober_manager.go:197] "Startup probe already exists for container" pod="kube-system/kube-controller-manager-minikube" containerName="kube-controller-manager"
Feb 09 18:41:44 minikube kubelet[2431]: E0209 18:41:44.716845    2431 prober_manager.go:197] "Startup probe already exists for container" pod="kube-system/kube-scheduler-minikube" containerName="kube-scheduler"
Feb 09 18:41:44 minikube kubelet[2431]: E0209 18:41:44.797912    2431 prober_manager.go:197] "Startup probe already exists for container" pod="kube-system/etcd-minikube" containerName="etcd"
Feb 09 18:41:46 minikube kubelet[2431]: E0209 18:41:46.797908    2431 prober_manager.go:197] "Startup probe already exists for container" pod="kube-system/kube-apiserver-minikube" containerName="kube-apiserver"
Feb 09 18:42:06 minikube kubelet[2431]: E0209 18:42:06.712887    2431 prober_manager.go:209] "Readiness probe already exists for container" pod="kube-system/coredns-7d764666f9-99hn5" containerName="coredns"
Feb 09 18:42:44 minikube kubelet[2431]: E0209 18:42:44.026171    2431 prober_manager.go:209] "Readiness probe already exists for container" pod="kube-system/coredns-7d764666f9-4nzzl" containerName="coredns"
Feb 09 18:43:00 minikube kubelet[2431]: E0209 18:43:00.020868    2431 prober_manager.go:197] "Startup probe already exists for container" pod="kube-system/kube-scheduler-minikube" containerName="kube-scheduler"
Feb 09 18:43:02 minikube kubelet[2431]: E0209 18:43:02.007062    2431 prober_manager.go:197] "Startup probe already exists for container" pod="kube-system/kube-controller-manager-minikube" containerName="kube-controller-manager"
Feb 09 18:43:03 minikube kubelet[2431]: E0209 18:43:03.006567    2431 prober_manager.go:197] "Startup probe already exists for container" pod="kube-system/etcd-minikube" containerName="etcd"
Feb 09 18:43:10 minikube kubelet[2431]: E0209 18:43:10.026912    2431 prober_manager.go:197] "Startup probe already exists for container" pod="kube-system/kube-apiserver-minikube" containerName="kube-apiserver"
Feb 09 18:43:18 minikube kubelet[2431]: E0209 18:43:18.528652    2431 prober_manager.go:209] "Readiness probe already exists for container" pod="kube-system/coredns-7d764666f9-99hn5" containerName="coredns"
Feb 09 18:44:04 minikube kubelet[2431]: E0209 18:44:04.672996    2431 prober_manager.go:197] "Startup probe already exists for container" pod="kube-system/kube-controller-manager-minikube" containerName="kube-controller-manager"
Feb 09 18:44:12 minikube kubelet[2431]: E0209 18:44:12.673275    2431 prober_manager.go:209] "Readiness probe already exists for container" pod="kube-system/coredns-7d764666f9-4nzzl" containerName="coredns"
Feb 09 18:44:20 minikube kubelet[2431]: E0209 18:44:20.382252    2431 prober_manager.go:197] "Startup probe already exists for container" pod="kube-system/kube-scheduler-minikube" containerName="kube-scheduler"
Feb 09 18:44:26 minikube kubelet[2431]: E0209 18:44:26.382880    2431 prober_manager.go:197] "Startup probe already exists for container" pod="kube-system/etcd-minikube" containerName="etcd"
Feb 09 18:44:26 minikube kubelet[2431]: E0209 18:44:26.382967    2431 prober_manager.go:209] "Readiness probe already exists for container" pod="kube-system/coredns-7d764666f9-99hn5" containerName="coredns"
Feb 09 18:44:30 minikube kubelet[2431]: E0209 18:44:30.382292    2431 prober_manager.go:197] "Startup probe already exists for container" pod="kube-system/kube-apiserver-minikube" containerName="kube-apiserver"
Feb 09 18:45:09 minikube kubelet[2431]: E0209 18:45:09.754750    2431 kubelet.go:2691] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.432s"
Feb 09 18:45:21 minikube kubelet[2431]: E0209 18:45:21.493283    2431 prober_manager.go:209] "Readiness probe already exists for container" pod="kube-system/coredns-7d764666f9-4nzzl" containerName="coredns"
Feb 09 18:45:28 minikube kubelet[2431]: E0209 18:45:28.102779    2431 kubelet.go:2691] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="5.226s"
Feb 09 18:45:28 minikube kubelet[2431]: E0209 18:45:28.112302    2431 prober_manager.go:197] "Startup probe already exists for container" pod="kube-system/kube-controller-manager-minikube" containerName="kube-controller-manager"
Feb 09 18:45:28 minikube kubelet[2431]: E0209 18:45:28.877054    2431 prober_manager.go:197] "Startup probe already exists for container" pod="kube-system/kube-scheduler-minikube" containerName="kube-scheduler"
Feb 09 18:45:33 minikube kubelet[2431]: E0209 18:45:33.876887    2431 prober_manager.go:197] "Startup probe already exists for container" pod="kube-system/etcd-minikube" containerName="etcd"
Feb 09 18:45:33 minikube kubelet[2431]: E0209 18:45:33.877121    2431 prober_manager.go:197] "Startup probe already exists for container" pod="kube-system/kube-apiserver-minikube" containerName="kube-apiserver"
Feb 09 18:45:38 minikube kubelet[2431]: E0209 18:45:38.679710    2431 prober_manager.go:197] "Startup probe already exists for container" pod="kube-system/kube-apiserver-minikube" containerName="kube-apiserver"
Feb 09 18:45:45 minikube kubelet[2431]: E0209 18:45:45.439535    2431 prober_manager.go:197] "Startup probe already exists for container" pod="kube-system/kube-apiserver-minikube" containerName="kube-apiserver"
Feb 09 18:45:46 minikube kubelet[2431]: E0209 18:45:46.148257    2431 prober_manager.go:209] "Readiness probe already exists for container" pod="kube-system/coredns-7d764666f9-99hn5" containerName="coredns"
Feb 09 18:45:50 minikube kubelet[2431]: E0209 18:45:50.342729    2431 kubelet.go:2691] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.532s"
Feb 09 18:46:28 minikube kubelet[2431]: E0209 18:46:28.380202    2431 prober_manager.go:197] "Startup probe already exists for container" pod="kube-system/kube-apiserver-minikube" containerName="kube-apiserver"
Feb 09 18:46:28 minikube kubelet[2431]: E0209 18:46:28.944336    2431 prober_manager.go:197] "Startup probe already exists for container" pod="kube-system/kube-apiserver-minikube" containerName="kube-apiserver"
Feb 09 18:46:31 minikube kubelet[2431]: E0209 18:46:31.276255    2431 prober_manager.go:209] "Readiness probe already exists for container" pod="kube-system/coredns-7d764666f9-4nzzl" containerName="coredns"
Feb 09 18:46:35 minikube kubelet[2431]: E0209 18:46:35.275976    2431 prober_manager.go:197] "Startup probe already exists for container" pod="kube-system/etcd-minikube" containerName="etcd"
Feb 09 18:46:41 minikube kubelet[2431]: E0209 18:46:41.275970    2431 prober_manager.go:197] "Startup probe already exists for container" pod="kube-system/kube-scheduler-minikube" containerName="kube-scheduler"
Feb 09 18:46:53 minikube kubelet[2431]: E0209 18:46:53.786706    2431 prober_manager.go:197] "Startup probe already exists for container" pod="kube-system/kube-controller-manager-minikube" containerName="kube-controller-manager"
Feb 09 18:46:53 minikube kubelet[2431]: E0209 18:46:53.788498    2431 prober_manager.go:209] "Readiness probe already exists for container" pod="kube-system/coredns-7d764666f9-99hn5" containerName="coredns"
Feb 09 18:46:54 minikube kubelet[2431]: E0209 18:46:53.940754    2431 event.go:359] "Server rejected event (will not retry!)" err="etcdserver: request timed out" event="&Event{ObjectMeta:{kube-apiserver-minikube.1892a86de5081828  kube-system    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Pod,Namespace:kube-system,Name:kube-apiserver-minikube,UID:3a13ff04419ee76498b33a80e312516a,APIVersion:v1,ResourceVersion:,FieldPath:spec.containers{kube-apiserver},},Reason:Unhealthy,Message:Liveness probe failed: HTTP probe failed with statuscode: 500,Source:EventSource{Component:kubelet,Host:minikube,},FirstTimestamp:2026-02-09 18:46:33.457440808 +0000 UTC m=+773.758373562,LastTimestamp:2026-02-09 18:46:33.457440808 +0000 UTC m=+773.758373562,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:minikube,}"
Feb 09 18:47:02 minikube kubelet[2431]: I0209 18:47:02.832421    2431 scope.go:122] "RemoveContainer" containerID="bc02f5535f6b6a77ec654204a1f59d66a16b4dbc7bd50da5e093c8d702a169f6"
Feb 09 18:47:31 minikube kubelet[2431]: E0209 18:47:31.541931    2431 prober_manager.go:209] "Readiness probe already exists for container" pod="ingress-nginx/ingress-nginx-controller-8675c6b56f-8xp54" containerName="controller"
Feb 09 18:47:32 minikube kubelet[2431]: E0209 18:47:32.553840    2431 prober_manager.go:209] "Readiness probe already exists for container" pod="ingress-nginx/ingress-nginx-controller-8675c6b56f-8xp54" containerName="controller"
Feb 09 18:47:33 minikube kubelet[2431]: E0209 18:47:33.566947    2431 prober_manager.go:209] "Readiness probe already exists for container" pod="ingress-nginx/ingress-nginx-controller-8675c6b56f-8xp54" containerName="controller"
Feb 09 18:47:34 minikube kubelet[2431]: I0209 18:47:34.620909    2431 pod_startup_latency_tracker.go:108] "Observed pod startup duration" pod="ingress-nginx/ingress-nginx-controller-8675c6b56f-8xp54" podStartSLOduration=170.573015917 podStartE2EDuration="10m11.398931939s" podCreationTimestamp="2026-02-09 18:37:23 +0000 UTC" firstStartedPulling="2026-02-09 18:39:50.553572643 +0000 UTC m=+369.282291434" lastFinishedPulling="2026-02-09 18:47:09.824873168 +0000 UTC m=+810.108207456" observedRunningTime="2026-02-09 18:47:34.292859397 +0000 UTC m=+834.555995785" watchObservedRunningTime="2026-02-09 18:47:34.398931939 +0000 UTC m=+834.662068327"
Feb 09 18:47:38 minikube kubelet[2431]: E0209 18:47:38.313628    2431 prober_manager.go:197] "Startup probe already exists for container" pod="kube-system/kube-apiserver-minikube" containerName="kube-apiserver"
Feb 09 18:47:45 minikube kubelet[2431]: E0209 18:47:45.325945    2431 prober_manager.go:197] "Startup probe already exists for container" pod="kube-system/kube-scheduler-minikube" containerName="kube-scheduler"
Feb 09 18:47:53 minikube kubelet[2431]: E0209 18:47:53.583746    2431 prober_manager.go:209] "Readiness probe already exists for container" pod="ingress-nginx/ingress-nginx-controller-8675c6b56f-8xp54" containerName="controller"
Feb 09 18:47:54 minikube kubelet[2431]: E0209 18:47:54.326274    2431 prober_manager.go:197] "Startup probe already exists for container" pod="kube-system/kube-controller-manager-minikube" containerName="kube-controller-manager"
Feb 09 18:47:55 minikube kubelet[2431]: E0209 18:47:55.326722    2431 prober_manager.go:197] "Startup probe already exists for container" pod="kube-system/etcd-minikube" containerName="etcd"
Feb 09 18:47:55 minikube kubelet[2431]: E0209 18:47:55.326991    2431 prober_manager.go:209] "Readiness probe already exists for container" pod="kube-system/coredns-7d764666f9-4nzzl" containerName="coredns"
Feb 09 18:47:58 minikube kubelet[2431]: E0209 18:47:58.425538    2431 controller.go:251] "Failed to update lease" err="etcdserver: request timed out"
Feb 09 18:47:59 minikube kubelet[2431]: E0209 18:47:59.119440    2431 controller.go:251] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"minikube\": the object has been modified; please apply your changes to the latest version and try again"
Feb 09 18:48:22 minikube kubelet[2431]: E0209 18:48:22.323852    2431 prober_manager.go:209] "Readiness probe already exists for container" pod="kube-system/coredns-7d764666f9-99hn5" containerName="coredns"
Feb 09 18:48:47 minikube kubelet[2431]: E0209 18:48:47.337643    2431 prober_manager.go:197] "Startup probe already exists for container" pod="kube-system/kube-scheduler-minikube" containerName="kube-scheduler"
Feb 09 18:49:04 minikube kubelet[2431]: E0209 18:49:04.653444    2431 kubelet.go:2691] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="3.329s"
Feb 09 18:49:04 minikube kubelet[2431]: E0209 18:49:04.668819    2431 prober_manager.go:209] "Readiness probe already exists for container" pod="ingress-nginx/ingress-nginx-controller-8675c6b56f-8xp54" containerName="controller"
Feb 09 18:49:06 minikube kubelet[2431]: E0209 18:49:06.327374    2431 prober_manager.go:197] "Startup probe already exists for container" pod="kube-system/kube-apiserver-minikube" containerName="kube-apiserver"
Feb 09 18:49:11 minikube kubelet[2431]: E0209 18:49:11.325717    2431 prober_manager.go:209] "Readiness probe already exists for container" pod="kube-system/coredns-7d764666f9-4nzzl" containerName="coredns"
Feb 09 18:49:21 minikube kubelet[2431]: E0209 18:49:21.331668    2431 prober_manager.go:197] "Startup probe already exists for container" pod="kube-system/etcd-minikube" containerName="etcd"
Feb 09 18:49:24 minikube kubelet[2431]: E0209 18:49:24.327482    2431 prober_manager.go:209] "Readiness probe already exists for container" pod="kube-system/coredns-7d764666f9-99hn5" containerName="coredns"
Feb 09 18:49:24 minikube kubelet[2431]: E0209 18:49:24.327618    2431 prober_manager.go:197] "Startup probe already exists for container" pod="kube-system/kube-controller-manager-minikube" containerName="kube-controller-manager"


==> storage-provisioner [8be3c06ec390] <==
I0209 18:48:02.454459       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_9cbe3aac-65b7-4f82-bd22-c4eaaa9f1975!
W0209 18:48:02.654505       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:04.659071       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:04.811098       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:07.730732       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:08.183261       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:10.188876       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:11.667165       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:15.590962       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:17.715816       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:19.722350       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:19.845175       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:21.851009       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:22.162017       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:24.938563       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:25.550932       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:27.715162       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:27.832079       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:29.838212       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:33.812732       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:36.810408       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:37.308558       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:39.317533       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:39.850520       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:41.858152       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:41.986321       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:43.992423       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:44.182374       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:46.806450       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:47.697251       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:49.724516       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:49.960168       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:51.968060       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:52.330039       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:54.337778       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:54.485397       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:56.491336       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:48:56.581666       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:49:06.474855       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:49:07.154674       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:49:09.163874       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:49:09.362328       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:49:11.367830       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:49:11.466810       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:49:13.516851       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:49:13.634423       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:49:15.690979       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:49:15.962789       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:49:18.053784       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:49:18.395810       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:49:20.400783       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:49:20.651321       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:49:22.824967       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:49:23.262248       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:49:25.600688       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:49:26.347659       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:49:28.352553       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:49:28.768689       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:49:32.607128       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0209 18:49:32.674512       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice


==> storage-provisioner [bc02f5535f6b] <==
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.3()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x5c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc0000120a0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x5f
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc0000120a0, 0x18b3d60, 0xc000028150, 0xc000076201, 0xc000420480)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc0000120a0, 0x3b9aca00, 0x0, 0x1, 0xc000420480)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc0000120a0, 0x3b9aca00, 0xc000420480)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x3d6

goroutine 91 [sync.Cond.Wait, 9 minutes]:
sync.runtime_notifyListWait(0xc00011c290, 0x3)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc00011c280)
	/usr/local/go/src/sync/cond.go:56 +0x99
k8s.io/client-go/util/workqueue.(*Type).Get(0xc0000763c0, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/util/workqueue/queue.go:145 +0x89
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).processNextClaimWorkItem(0xc000164780, 0x18e5530, 0xc0004de980, 0x203000)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:935 +0x3e
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).runClaimWorker(...)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:924
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.2()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:880 +0x5c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc0000120c0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x5f
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc0000120c0, 0x18b3d60, 0xc000028180, 0x1, 0xc000420480)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc0000120c0, 0x3b9aca00, 0x0, 0x1, 0xc000420480)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc0000120c0, 0x3b9aca00, 0xc000420480)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:880 +0x4af

goroutine 92 [sync.Cond.Wait, 9 minutes]:
sync.runtime_notifyListWait(0xc00011c2d0, 0x3)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc00011c2c0)
	/usr/local/go/src/sync/cond.go:56 +0x99
k8s.io/client-go/util/workqueue.(*Type).Get(0xc000076540, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/util/workqueue/queue.go:145 +0x89
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).processNextVolumeWorkItem(0xc000164780, 0x18e5530, 0xc0004de980, 0x203000)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:990 +0x3e
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).runVolumeWorker(...)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:929
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.3()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x5c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc0000120e0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x5f
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc0000120e0, 0x18b3d60, 0xc000614060, 0x1, 0xc000420480)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc0000120e0, 0x3b9aca00, 0x0, 0x1, 0xc000420480)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc0000120e0, 0x3b9aca00, 0xc000420480)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x3d6

